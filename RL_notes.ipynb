{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d87e49f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Iterative Application of the Bellman Equation in Gridworld (Deterministic)\n",
    "\n",
    "The **iterative application of the Bellman Equation** is the key to solving value functions in Reinforcement Learning. In a deterministic environment, the Bellman Equation is applied repeatedly to each state until the value function converges. This notebook provides a detailed breakdown of the process.\n",
    "\n",
    "---\n",
    "\n",
    "## Bellman Equation Recap\n",
    "\n",
    "For a deterministic environment, the Bellman Equation for a state $s$ under a given policy $\\pi$ is:\n",
    "\n",
    "$$\n",
    "V(s) = R(s, \\pi(s)) + \\gamma V(s')\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $V(s)$: Value of state $s$.\n",
    "- $R(s, \\pi(s))$: Immediate reward for taking action $\\pi(s)$ in state $s$.\n",
    "- $s'$: The next state reached by following $\\pi(s)$.\n",
    "- $\\gamma$: Discount factor ($0 \\leq \\gamma \\leq 1$).\n",
    "\n",
    "The iterative method involves repeatedly applying this equation to update the value of each state.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Iterative Process\n",
    "\n",
    "### Step 1: Initialize Values\n",
    "- Initialize $V(s)$ for all states $s$. A common choice is $V(s) = 0$ for all states.\n",
    "\n",
    "### Step 2: Apply the Bellman Equation\n",
    "- For each state $s$ (except terminal states):\n",
    "  1. Use the current value of $V(s')$ for the next state $s'$.\n",
    "  2. Update $V(s)$ using the Bellman Equation.\n",
    "\n",
    "### Step 3: Repeat\n",
    "- Repeat Step 2 for multiple iterations, updating $V(s)$ based on the values from the previous iteration.\n",
    "\n",
    "### Step 4: Convergence\n",
    "- The process stops when the values $V(s)$ stop changing significantly (convergence), indicating that the value function is stable.\n",
    "\n",
    "---\n",
    "\n",
    "## Gridworld Example in Detail\n",
    "\n",
    "### Environment Setup\n",
    "- **Grid Size**: 3x3\n",
    "- **Rewards**: $-1$ per step, $+10$ at the goal ($s_9$).\n",
    "- **Policy**: Move toward the goal in the shortest path.\n",
    "- **Discount Factor**: $\\gamma = 0.9$.\n",
    "\n",
    "### Policy\n",
    "The deterministic policy $\\pi(s)$ maps each state to an action (e.g., right, down):\n",
    "$$\n",
    "\\pi(s_1) = \\text{right}, \\quad \\pi(s_2) = \\text{right}, \\quad \\pi(s_3) = \\text{down}, \\quad \\dots\n",
    "$$\n",
    "\n",
    "### Initial Values\n",
    "$$\n",
    "V(s_1) = V(s_2) = \\dots = V(s_9) = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Iteration-by-Iteration Example\n",
    "\n",
    "### Iteration 0\n",
    "All values are initialized to $0$:\n",
    "$$\n",
    "V(s) = 0 \\quad \\forall s\n",
    "$$\n",
    "\n",
    "### Iteration 1\n",
    "Apply the Bellman Equation to each state $s$ using the initial values.\n",
    "\n",
    "For $s_1$ (move right to $s_2$):\n",
    "$$\n",
    "V(s_1) = R(s_1, \\pi(s_1)) + \\gamma V(s_2) = -1 + 0.9 \\cdot 0 = -1\n",
    "$$\n",
    "\n",
    "For $s_2$ (move right to $s_3$):\n",
    "$$\n",
    "V(s_2) = -1 + 0.9 \\cdot 0 = -1\n",
    "$$\n",
    "\n",
    "For $s_3$ (move down to $s_6$):\n",
    "$$\n",
    "V(s_3) = -1 + 0.9 \\cdot 0 = -1\n",
    "$$\n",
    "\n",
    "Repeat for all states. The updated values are:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "-1 & -1 & -1 \\\\\n",
    "-1 & -1 & 10\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Iteration 2\n",
    "Use the updated values from Iteration 1:\n",
    "\n",
    "For $s_1$ (move right to $s_2$):\n",
    "$$\n",
    "V(s_1) = R(s_1, \\pi(s_1)) + \\gamma V(s_2) = -1 + 0.9 \\cdot (-1) = -1.9\n",
    "$$\n",
    "\n",
    "For $s_2$ (move right to $s_3$):\n",
    "$$\n",
    "V(s_2) = -1 + 0.9 \\cdot (-1) = -1.9\n",
    "$$\n",
    "\n",
    "For $s_3$ (move down to $s_6$):\n",
    "$$\n",
    "V(s_3) = -1 + 0.9 \\cdot (-1) = -1.9\n",
    "$$\n",
    "\n",
    "The updated values are:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1.9 & -1.9 & -1.9 \\\\\n",
    "-1.9 & -1.9 & -1 \\\\\n",
    "-1.9 & -1 & 10\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Subsequent Iterations\n",
    "The values continue to propagate back toward the starting states. For example:\n",
    "\n",
    "- In Iteration 3, $V(s_1)$ considers the updated value of $V(s_2)$.\n",
    "- This backward propagation continues until all values converge.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Values\n",
    "After convergence, the values represent the cumulative discounted reward for following the policy:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-3.44 & -2.71 & -1.90 \\\\\n",
    "-2.71 & -1.90 & -1.00 \\\\\n",
    "-1.90 & -1.00 & 10.00\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Iterative Propagation**:\n",
    "   - The value of each state propagates backward from the goal state through repeated applications of the Bellman Equation.\n",
    "\n",
    "2. **Convergence**:\n",
    "   - The values stabilize after a sufficient number of iterations, representing the long-term expected reward.\n",
    "\n",
    "3. **Deterministic Simplicity**:\n",
    "   - In deterministic environments, the Bellman Equation simplifies to a single expected future state, making the iterative process computationally efficient.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b836b",
   "metadata": {},
   "source": [
    "# Pseudo-Code: Iterative Application of the Bellman Equation\n",
    "\n",
    "This pseudo-code describes the iterative process for applying the Bellman Equation to compute the value function $V(s)$ for a deterministic policy in a gridworld.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs:\n",
    "- **Gridworld**: A grid with states and possible transitions.\n",
    "- **Policy**: A mapping from states to actions, $\\pi(s)$.\n",
    "- **Rewards**: Reward $R(s, \\pi(s))$ for each state-action pair.\n",
    "- **Discount Factor**: $\\gamma$ (a value between 0 and 1).\n",
    "- **Number of Iterations**: Maximum iterations for value updates.\n",
    "\n",
    "---\n",
    "\n",
    "## Pseudo-Code:\n",
    "\n",
    "1. **Initialize**:\n",
    "   - Set $V(s) = 0$ for all states $s$.\n",
    "\n",
    "2. **For each iteration**:\n",
    "   - Create a copy of the current value function: $V_{\\text{new}} = V$.\n",
    "   - For each state $s$ (excluding terminal states):\n",
    "     - Determine the action $a = \\pi(s)$ according to the policy.\n",
    "     - Compute the next state $s'$ based on the action $a$.\n",
    "     - Update the value of the state $s$ using the Bellman Equation:\n",
    "       $$\n",
    "       V_{\\text{new}}(s) = R(s, a) + \\gamma V(s')\n",
    "       $$\n",
    "\n",
    "3. **Update Values**:\n",
    "   - Set $V = V_{\\text{new}}$.\n",
    "\n",
    "4. **Repeat Until Convergence or Maximum Iterations**:\n",
    "   - Stop if the values $V(s)$ for all states change by less than a small threshold (e.g., $\\epsilon = 10^{-6}$).\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs:\n",
    "- $V(s)$: The converged value function representing the cumulative discounted reward for following the given policy.\n",
    "\n",
    "---\n",
    "\n",
    "## Example in Gridworld:\n",
    "For a 3x3 grid:\n",
    "- **Rewards**: $R(s, a) = -1$ for all states except the goal state, where $R(s, a) = +10$.\n",
    "- **Policy**: Move toward the goal in the shortest path.\n",
    "- **Discount Factor**: $\\gamma = 0.9$.\n",
    "\n",
    "After running the iterative process, the value function $V(s)$ converges to:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-3.44 & -2.71 & -1.90 \\\\\n",
    "-2.71 & -1.90 & -1.00 \\\\\n",
    "-1.90 & -1.00 & 10.00\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1baf7ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAAB0CAYAAADdJNT+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5TklEQVR4nO3dd3hUZf7+8fczmfTeQwARpQlKE0FBjWJjwcZaUBEXRcVe17qsshawK6JrBbFg/4qugshPEBAUFAHpvUNIr4T08/vjxBBIm5hMZhLv13Vxkcw853zOc+5JMvOZc84Yy7IQERERERERERHPc3h6A0RERERERERExKZGjYiIiIiIiIiIl1CjRkRERERERETES6hRIyIiIiIiIiLiJdSoERERERERERHxEmrUiIiIiIiIiIh4iVbZqDHG5BtjjvH0dkh1ysZ7KRvvpWy8l7LxXsrGeykb76VsvJey8V7Kxnu0piyavFFjjNlhjDm74uvRxphFTV3jiHrzjTHXV73NsqwQy7K2uaFWlDFmhjHmgDFmpzHmqqau4U6tPJvbjDHLjDFFxphpTb1+d2ut2Rhj/I0xUyp+XvKMMSuNMX9ryhru1lqzqaj1gTEm2RiTa4zZdGRdb9eas6lSs7MxptAY84G7arhDa86molZhxZPBfGPMxqau4U6tOZuKelcYY9ZXPFfbaow5zR113KE1Z1Pl5+WPf2XGmMlNXcddWnk2RxtjZhljsowx+40xrxhjnE1dx11aeTbHGWPmGWNyjDFbjDHDm7pGU2rlWdT5WtMYc5YxZoMxpsAY84MxpkNja3r1ETVe+EviVaAYiAdGAq8ZY3p4dpM8wwuz2Qc8AUz19IZ4mpdl4wR2A0lAODAO+NQYc7QnN8pTvCwbgInA0ZZlhQEXAk8YY0708DZ5hBdm84dXgV89vRGe5KXZ3FbxZDDEsqyunt4YT/G2bIwx5wBPA9cCocDpgNsaqd7M27Kp8vMSAiQAB4HPPLxZHuFt2QD/BVKBNkBv7Odst3hygzzFm7Kp2JavgG+AKOBG4ANjTBePblgz8aYsKtT6WtMYEwN8AfwbO6tlwCeNrmhZVpP+A3YAZwPHAYVAGZAPZFfc7w88B+wCUoDXgcCK+84A9gAPAPuB94FI7AdoGpBV8XW7ivFPVqy/sKLGKxW3W0Cniq/Dgfcqlt+J/ULRUXHfaGBRxfZkAduBv9Uyr2DsJk2XKre9DzzV1PvQXf9aazZHzPEJYJqn97WyqXOuq4BLPL3PlU21eXYFkoHLPb3PlU3l/K4APgXGAx94en8rm8q5zQeu9/Q+VjY1zu0nYIyn97GyqXee/8BuoBlP73NlYwGsB4ZW+f5Z4A1P7/O/ejbA8RU1TJXb5gCPe3qf/9WyOGKO1V5rYjfRfqryfTB2M7pbY/an246osSxrPXAT8LNld9AjKu56CuiC3bHtBLQFHqmyaAJ2J6pDxaQdwDsV3x9VMelXKmr8C/iRQ+9s3VbDpkzGDukY7A7xNdjvtPxhALARiAGeAaYYY0wN6+kClFqWtanKbb8DLe6ImlaYTavR2rMxxsRXzGNtfWO9TWvNxhjzX2NMAbABu1Ezq/694V1aYzbGmDDgMeAeF3eDV2qN2VSYaIxJN8YsNsacUd9+8EatLRtjjA/QD4itOEVgT8UpHIEN2C1eobVlU4N/AO9ZFa9oWpJWms1LwBXGmCBjTFvgb8Ds+veGd2ml2RzJYDdwvNpfJIuqemD3Bf6Y/wFgK43tE7irk1a1U1XlPgMcAI6tctspwHbrUCetGAioY/29gawq38/niHe2qOikAT4V6+te5b6xwPwq27elyn1BFcsm1FD3NGD/Ebfd8Me6WsK/1prNEetv0UfUtPJsfIHvaUHv0vyFsvEBTsV+p8HX0/tc2VgAk4AHKr4eTws9oqaVZjMA+7Qaf+wXnHlV5+Lt/1prNkBixX3LsE/hiAEWA096ep//1bM5Yv0dsN8F7+jp/a1sKu8/DvgNKK0YN40WeLRTa8sG+3nzNuD+iq/PrVj3d57e53+1LI5Yf01H1EzhiLNssP/+jG7M/nTSvGIrdsJvVZpVBntH/iHNsqzCyjuNCQJeBIZgH/4EEGqM8bEsq6yeejHYD+ydVW7bid29+8P+P76wLKugYrtCalhXPhB2xG1h2E/QWoOWnE1r1+KzMcY4sA9hLAZq6ni3VC0+m4pxZcAiY8zVwM3Ay/VsR0vQYrMxxvTGPnS4Tz01W6oWm03F/UurfPuuMeZKYCj2O3ctXUvO5mDF/5Mty0qu2LYXsBvQ/6pnO1qClpxNVaOwX7xtr2dcS9Jis6l4fjYbeBMYWDFmKva1nu6vZztaghabjWVZJcaYi7H/tjyA3YT+FCiqZxu8VYvNwgVu6RO4+2LC1hHfp2P/Ie1hWVZExb9wy76wWG3L3It97YQBln3By9Mrbje1jD+yXgl29/4PRwF7GzCHP2wCnMaYzlVu60ULPIWjQmvKprVpVdlUHEI4Bfsi3JdYllXyZ9bjJVpVNjVwAsc20bqaW2vK5gzgaGCXMWY/8E/gEmPM8j+xLm/QmrKpiVVlO1qaVpONZVlZ2Nc3qFqvrtrertVkc4RrgHcbuQ5Pa03ZRFUs+4plWUWWZWVgn2oy9E+syxu0pmywLGuVZVlJlmVFW5Z1HvYpPL/8mXV5QKvKoh5rsfsCABhjgrGfTzeqT+DuRk0K0M4Y4wdgWVY58BbwojEmDsAY09YYc14d6wjFDjXbGBMFPFpDjRo/K72i0/Yp8KQxJrTiY7LuAT5o6EQs+1yzL4DHjDHBxphBwEXYRwm0RK0mm4ptdRpjArC7sj7GmAAvvFq4q1pVNsBr2IfVXmBZ1sH6Bnu5VpONMSbO2B9jG2KM8anY5iuBuQ1dl5doNdlgv7N5LPYhvr2xL7Y3E6hr271Zq8nGGBNhjDnvj78xxpiR2E8cW9z1HCq0mmwqvAPcXvH7LRK4G/viky1Ra8sGY8xA7HezW/qnPbWabCzLSse+iOrNFb/TIrBP6VzV0HV5iVaTTcW29qz4exNkjPkn9mmd0/7MujygtWVR12vNGcDxxphLKsY8AqyyLGvDn6n1B3c3auZhd5L2G2PSK257ANgCLDHG5GJfs6JrHet4CQjE7ootofqToUnApcaYLGNMTYfr3459Ptw27Cs7f8if/wjnWyq2JRX4CLjZsqyWekRNa8tmHPYP8oPA1RVfj/uT6/K0VpNNxS/FsdgvNvcbY/Ir/o1s6Lq8RKvJBvtdiJux34HOwr7q/V2WZf3vT6zLG7SabCzLKrAsa/8f/7APqS20LCutoevyEq0mG+zDqJ/A/gSJ9Ir1Xmwd/kEDLUlrygbgceyPs9+E/Uk2K7A/GaQlam3ZgN0A+MKyrJZ+2YDWls3fsU8tSauYQwl2k7Mlam3ZjML+oIdU4CzgHMuyWsqpT60ti1pfa1Y8P7sE++9NFva17K74k3UqGctqyUeFioiIiIiIiIi0Hu4+okZERERERERERFykRo2IiIiIiIiIiJdQo0ZERERERERExEuoUSMiIiIiIiIi4iXUqBERERERERER8RLOuu4cdMECfSSUGyz+Osk0dh3Kxj2UjfdSNq4Zc/fpzV7zusGmRWZzzuUDm7XeRQNymrUeQJ8uMcrGBcrGdcrGNcrGPZSNa5SN65SNa5SNe9SVjY6oERERERERERHxEmrUiIiIiIiIiIh4CTVqRERERERERES8hBo1IiIiIiIiIiJeQo0aEREREREREREvoUaNiIiIiIiIiIiXUKNGRERERERERMRLqFEjIiIiIiIiIuIl1KgREREREREREfESzoYukJW6lO1rXwGrjLijhtGu08jD7i8vK2bzyokcyNmI0y+cLn0fISCoDQB7tkwndddMMD507HE7kXH9vbKmJ+bYFAryd7Jl5dMcyN3MUV3H0PbYK2ocV1iQzKblj1FanENweFc693kYh8O3znl5S01PzLEpuLrdOenL2bHuNcqtEkLCu9Kp530YhxPLsti+djLZqUtw+ATQqfeDhIR38aqanphjU0jb8//Yu/UjwMLHGcQxJ9xNcFinauNW/3Q7ZaUFAJQUZRMa0Y1uJz3p8rw9WTN51wamPnMdu7YsZ/h1TzDk8n/WOG79inl8+vp9lJYWc3TnExl939v4+NjZfPTqnaxe+i1+/kFcd/87dOjSt955Nlbm/kXs2jgVjMEYHzr2uI2wqJ61jl//68MUFuyjT9K0w27fu/UTdq5/jZPO/RJfv4g6a6766UMWz3wOLAu/gFCGjZ5MwlG9qo1754kzKSrMA+BAbhptj+nHFXf9H+n7NvDVWzeQvHMFgy99jIFD76mz3vatmxn/8B1sWLuKW+9+mGvG3FrjuF9+/pGXnnmUkpISjuvRk0eenITTaWfz7JMPs2jB9wQEBPGfp17muB7Vt7epKZtDlI2ycZWyOUTZKBtXKZtDlM1fK5sGNWosq4xtaybRY8Bz+AXGsurHm4iKH0RQ6NGVY1J2z8LpG0LfwR+SvncuO9e/SdcTH6Ugbwfpe+fRO2kaxUUZrF1yL33PfB9jfLyqpifm2FScvmF0PP4OMvcvqnPczvVvkNjxUmLansXWVc+TumsWCUdfVOu8vKmmJ+bYFFzZbssqZ/PKifQ4+QUCQ9qza+NUUvd8R/xRw8hOXUrhgT30OXM6+dnr2Lb6RXqe+ppX1fTEHJuCf1Abjj9lEk6/ULJSl7J11fM11j1h4OTKrzcse4SohEGA649JT9YMDo3iqtsmsWLxl7WOKS8vZ8rTo/nns9+T0L4LX77zCD999y6nDR3D6l++JWXPFia8t4lt65fy/qRbGPfqEpfn+2eFx/SlV/wgjDEcyN3Kpt/G0+fM92scm5G8EB+fwGq3Fx1MJSd9GX6B8S7VjIztyOiH5xIYHMnm32fzzdRbuH784mrjrh33Q+XXn758OV37XgBAYEgUQ0a9yIbfvnKpXnhEBPf/awI/zJ1V65jy8nIeffA2Xp/2BR06Hstrk57imxkfc/FlV7N44ffs2rGNr+b8wurff2Pi+Pt577PvXKrdGMrGpmxsysY1ysambGzKxjXKxqZsbH+lbBp06lN+9gYCg9sSEJyIw+FLTNvBZKYcvmOyUhYT134IANFtkshJ/w3LsshMWUxM28E4fPwICGpDYHBb8rM3eF1NT8yxqfj5RxIa0a2eRpRFTvpyotskARDXfgiZKfaLvdrm5U01PTHHpuDKdpcW52IcvgSGtAcgIqYfGckLAchMWUxsu/MwxhAa2YPSknyKCzO8qqYn5tgUwqKOx+kXCkBoRHeKD6bVOb605AA5GcuJij8VcG3enq4ZFhlHx24n4eP0rXXMgdwMnE4/EtrbRzF1P/EcfvvxCwBWLv6KgeeOwhjDsd1PpiA/m+yMZJdqN4aPMwhjDADlZYVQ8fWRykoL2LftU9p1HlXtvu1rX6HDcWOpecnq2nc+hcDgSADadRpAbtbeOscXHcxl+7r5dDvxIgCCw+Joe0w/fHxq39dVRUXH0qNnH5x1ZJOTnYmvrx8dOh4LwIBBScyd8w0A8+fO5vyLR2CMoWfvfuTl5pCWut+l2o2hbGzKxqZsXKNsbMrGpmxco2xsysb2V8qmQY2aooNp+AXEVn7vFxBb7QVGUeGhMcbhxMc3hNKSHIoPpuF/xLJF9bw48URNT8yxOZWW5OD0DcE47IOp/AJiKSq0t7G2ebW0mp6YY1Nw+oVjWWWVzb2M5AUUF6YCUFyYhn/goceWf0AsxYWNf2w1d01PzLEhUnbPJKKe0xUzUxYRHt0Xp29wi61Zk5DwGMrKStmxcRkAyxZ+TmbabgCy0vcRFdu+cmxkbDuy0+v+w9hUMpJ/ZMUPo1j/y4N06vVAjWN2bZxK4jEjcPj4H3Z75v5F+AfE1nhamStWLHiHTj3Pq3PMht++omOPM/EPDPtTNVwRERlNaVkp61avBGDu7K9J2b8PgNSUZOITEivHxiUkkpbi/idnoGxA2dRE2dRN2SibmiibuikbZVOT1p6NLiYs4iWMMXTt+wjb173Kqh9vwscZCMa9P6LNXdMTc3RVTvoKUnfPosNxY+scl753LrFtz2qxNWtjjGHsuI/4+L/38MQtAwgICsXhaJ7TNusS3eY0+pz5Pl37PcGujVOq3X8gZzOFBfuIbnPaYbeXlRWyZ8t02ne99k/V3b5uPisWvMPZl0+oc9yaJZ9y/Mkj/lQNVxljmPjCmzw3cRyjLj2XoOAQHA7P/9woG2VzJGVTP2WjbI6kbOqnbJTNkf4K2TToGjX+gYe/w11cmIZflXfA4dC74P6BcVjlpZSV5OP0Dccv8NBRDX8s63/Est5Q0xNzbIzkHTNI2WUfWtW9/9P4BcTUOd7pG05pST5WeSnG4bS3seIIk9rm5emanphjU2jodgOERvaovC5JdtqvHDywB6h+dFbVI4M8WdMTc2wKR253SXEOW1Y9S/f+T+PrV/vjoaQ4m/zsDXTr97jX15z35assnPU2AHdOmElkTGI9S0CnHqfw4CT7VLQ1y+aQsmczAJExiZVH1wBkpe0hIqZtg7bHVbU9psKje7Hl92RKirMPu9BcXtY68rM38tvcEVhWGSVF2az56U46Hn8nhQXJ/L5wDGA/nn5feCM9T30Nv4Dow2r+8v1rLJ9vP7kYee//KMhL5+upNzHy3v8RFHr42KoK8tLZu/VXRtzxWYPm+Mn0Kcz41D6He/KbHxMbn1DvMr36nMTUD+398vOiH9i1YysAcfFtKt+1AUjdv8+l9f0ZyqZmykbZ1EXZ1EzZKJu6KJuaKZu/VjYNatSEhHfl4IE9FBYk4xcQQ/reeXTpO+6wMZHxA0ndPZvQyB5kJC8gPKYvxhii4geyafkTJHa8jOKiDA4e2ENIRDevq+mJOTZGm6OH0+bo4S6PN8YQHtOHjOQFxLQ9i9Tds4mMty9QWtu8PF3TE3NsCg3dboDioiz8/CMpLytm75aPaNf5asDe7v07ZhCTOJj87HU4ncHVfol5oqYn5tgUqm530cEUNi77N517P1x57ZzaZCQvIDL+lGqHbnpjzcEX38rgi2u+Mn1tcrNSCYuMo6S4iG8/fobzRz4MQK+BFzLvy1fpf+YVbFu/lKDgcCKi3fNpaVX308EDe7AsC2MM+TmbsMpKqjVWE46+iISj7fOOCwuSWf/rQxw/cBIA/c/9snLcb3NH0PO0N2r8NIH+Z99M/7NvBiAnfRefvDyC4WPfIbpN3Z86tu7XL+jSeyhOv4AGzXHEyDGMGDmmQctkZqQRFR1LcXER096azJib7gYgafB5fPLBFM4bNpzVv/9GSGgYsXHueXKmbGqmbJRNXZRNzZSNsqmLsqmZsvlrZdOgRo1xODmmx52sW3ofllVOfPu/ERTakV0bpxIS3pWohEHEtx/K5pUTWD7vKpy+YXTp+wgAQaEdiUk8gxULRmOMD8ccf5dLF8Js7pqemGNTKS7MYNWisRUf52tI3v45vZPexekbzLqlD9Cp1334BcTQodtYNi1/jF0bpxAc3pn49kMBap2XN9X0xBybgqvbvW/rx2Sl/oxlWSR0uJDwGPsjkCPjTiY7dSnLfxiJj49/reeAerKmJ+bYFHZvepeSkly2rXkRAGN86HXamwCHbTdA+t55tO10lcvz9paaOZn7efzmkzhYkIsxDr7/v0k8PnUtgcFhvPTQMP5x71tExiQy+9NnWbVkJuXl5Zx54U0c12cwAD0HDGX10lk8NKozfgFBXHff1Abs4T8vI3khaXvmYBw+OBz+dDnxkcrG6sqFY+h9evVDbBtrwVdPcjA/g5nv3g6Aw+HkxsfsT7ia/tyFXDjmdUIj7aOT1iz5lFPPv++w5fOz9/Pmo6dQdDAX43Cw5LvJ3PrU77WeG52elsLVl5zDgfw8jMPBh+++weezFhMSEsrtN1zBI0+8RGx8Au++/So/zp+DVV7OpVeOpv8p9iHEpyadw6IF33PROf0JCAxk/ISXm3yf1ETZKJuqlI1rlI2yqUrZuEbZKJuq/krZmLo+8WbQBQvc/3E4f0GLv05q9CEcysY9lI33UjauGXP36c1e87rBjT8szRPZnHP5wGatd9GA5r9weZ8uMcrGBcrGdcrGNcrGPZSNa5SN65SNa5SNe9SVjeevQCQiIiIiIiIiIoAaNSIiIiIiIiIiXkONGhERERERERERL6FGjYiIiIiIiIiIl1CjRkRERERERETES6hRIyIiIiIiIiLiJdSoERERERERERHxEmrUiIiIiIiIiIh4CTVqRERERERERES8hBo1IiIiIiIiIiJewunpDRCRluftFzt5ehNajDF3n96s9Ub4ft6s9WyXNXoN51w+sAm2o2EuGpDTrPW6rXynWesB0OW+Rq9C2biJsnGJsnGdsnGNsnETZeMSZeO6v3o2OqJGRERERERERMRLqFEjIiIiIiIiIuIl1KgREREREREREfESatSIiIiIiIiIiHgJNWpERERERERERLyEGjUiIiIiIiIiIl5CjRoRERERERERES+hRo2IiIiIiIiIiJdwNnSBrNSlbF/7ClhlxB01jHadRh52f3lZMZtXTuRAzkacfuF06fsIAUFtANizZTqpu2aC8aFjj9uJjOvvlTU9McemUJC/ky0rn+ZA7maO6jqGtsdeUeO4woJkNi1/jNLiHILDu9K5z8M4HL51zstbanpijk3B1e3OSV/OjnWvUW6VEBLelU4978M4nFiWxfa1k8lOXYLDJ4BOvR8kJLyLV9XctnUL/3rwHtatXcOd99zPddffVOO4Pbt3ce9dt5CdnUWP43vy1LOT8PPzo7ioiAfvv4u1a1YRERHJC5Neo2279nXOsSmk7fl/7N36EWDh4wzimBPuJjisU7Vxq3+6nbLSAgBKirIJjehGt5OedHk/V5W8awNTn7mOXVuWM/y6Jxhy+T9rHLd+xTw+ff0+SkuLObrziYy+7218fOxsPnr1TlYv/RY//yCuu/8dOnTpW2u9jTv3cvNT/2Xlpu08ev0V3HnlhTWO27EvldH/eYnM3Dx6dzmGt8fdjp+vk6LiEm548hVWbtpGVFgo746/iw5t4uqdZ2Ot+ulDFs98DiwLv4BQho2eTMJRvaqNe+eJMykqzAPgQG4abY/pxxV3/R/p+zbw1Vs3kLxzBYMvfYyBQ++pt+b2rZsZ//AdbFi7ilvvfphrxtxa47hffv6Rl555lJKSEo7r0ZNHnpyE02ln8+yTD7NowfcEBATxn6de5rge1bf5Dxv3pjL2tU9ZuX0v468Ywl0XJNU4bkdqJtdMmk5mXgF9jmnLlNuuwM/ppKiklOtf/ZgV2/YSFRrE+3eOpENcVL3zbCxlc4iyUTauUjaHKBtl4yplc4iyaf5swHP5NOiIGssqY9uaSXTv/zS9z3iX9L3zKMjbcdiYlN2zcPqG0HfwhyR2vJSd698EoCBvB+l759E7aRrdBzzDtjUvYVllXlfTE3NsKk7fMDoefweJx4yoc9zO9W+Q2PFS+g7+EKdvCKm7ZgG1z8ubanpijk3Ble22rHI2r5xIl76P0CdpGv6B8aTu+Q6A7NSlFB7YQ58zp3Nsz3vZtvpFr6sZHhHBw/9+jGuvH1vnuOefncA/rr2B7+YuJiwsnC8++xiA//v8Y8LCwvlu7mL+ce0NPP/shHrn2BT8g9pw/CmT6J30Du06X8PWVc/XOO6EgZPpffoUep8+hdDIHkS1OR1w/TFZVXBoFFfdNonzLru31jHl5eVMeXo0Y8d9xONTVhMdfxQ/ffcuAKt/+ZaUPVuY8N4mrrnnDd6fdEud9SLDQnj2jmu544oL6hz37zc+4NbLh7Hqo8lEhAbz7sx5ALw7cx4RocGs+mgyt14+jH+/Pt3luTZGZGxHRj88l5snrOD0ix7mm6k1z/PacT9w0xPLuOmJZbTvNIDj+l0MQGBIFENGvcgpf7vb5ZrhERHc/68JjBpT+z4tLy/n0QdvY+ILb/HZNz/SJrE938ywH8eLF37Prh3b+GrOL4x7/Hkmjr+/7jmGBPHc6Iu4s5Y/+n8YN30Wtw89jTUvP0BEcCDT5v0KwLR5vxARHMialx/g9qGnMe7DWS7PtTGUzSHKRtm4StkcomyUjauUzSHKpvmzAc/l06BGTX72BgKD2xIQnIjD4UtM28Fkpiw+bExWymLi2g8BILpNEjnpv2FZFpkpi4lpOxiHjx8BQW0IDG5LfvYGr6vpiTk2FT//SEIjumGMT61jLMsiJ3050W3sB1pc+yFkpiwCap+XN9X0xBybgivbXVqci3H4EhhiH0USEdOPjOSFAGSmLCa23XkYYwiN7EFpST7FhRleVTM6OoYTevbG6az9QD3Lsli6ZDHnDhkGwMV/v4y539uNoXnfz+Hiv18GwLlDhrHk50XNkk1Y1PE4/UIBCI3oTvHBtDrHl5YcICdjOVHxpwKu7edqNSPj6NjtJHycvrWOOZCbgdPpR0J7+yim7ieew28/fgHAysVfMfDcURhjOLb7yRTkZ5OdkVzruuIiwznxuE74+tT9c7Ng+VqGJ50MwMghZ/DNj/YfmJmLljFyyBkADE86mfnL1zRLNu07n0JgcCQA7ToNIDdrb53jiw7msn3dfLqdeBEAwWFxtD2mHz4+te/nI0VFx9KjZx+cdWSTk52Jr68fHToeC8CAQUnMnfMNAPPnzub8i0dgjKFn737k5eaQlrq/1nXFhYfQr1N7fH1q/3NsWRYL1m5h+MknAHB1Uj+++XUtADOXrePqpH4ADD/5BOav2aJslI2yQdn8Qdm4RtnYlI1N2bjmr5ANeC6fBjVqig6m4RcQW/m9X0BstRc1RYWHxhiHEx/fEEpLcig+mIb/EcsW1fOCyBM1PTHH5lRakoPTNwTjsF9M+wXEUlRob2Nt82ppNT0xx6bg9AvHssoqm3sZyQsoLkwFoLgwDf/AQ48t/4BYigsb/9hq7prZWVmEhoZVNnPiE9qQkmL/ckxJ2U9Cgn0amtPpJDQkjOysrEbVa6iU3TOJqOd0xcyURYRH98XpG+zWbQkJj6GsrJQdG5cBsGzh52Sm7QYgK30fUbGHTguLjG1Hdnrdfxzrk5GTR0RIEE6n3cxpGxvFvvRMAPalZ9IuLhoAp9OH8OAgMnLyGlWvoVYseIdOPc+rc8yG376iY48z8Q8Mc+u2RERGU1pWyrrVKwGYO/trUvbvAyA1JZn4hMTKsXEJiaSl1P0EoD4ZeQWEBwXirGi0tY0KZ1+m/XtrX2YObaPDAXD6+BAWFEBGXkGj6jWUslE2rlA2h1M2ysYVyuZwykbZuKK5swH35NPga9SIiHsYY+ja9xG2r3sVq6yEiNh+YNx7vW9P1PRWOekrSN09i+MHTq5zXPreucQfNczt22OMYey4j/j4v/dQWlJE937n4HC4ftROa7J93XxWLHiHa8fNr3PcmiWf0ifpWrdvjzGGiS+8yXMTx1FSXMzJg87A4fhr/twoG++lbLyXsvFeysZ7KRvvpWzco0GNGv/Aw99RLy5Mw6/KO+5w6F13/8A4rPJSykrycfqG4xd46KiGP5b1P2JZb6jpiTk2RvKOGaTssg/l6t7/afwCYuoc7/QNp7QkH6u8FONw2ttYcYRJbfPydE1PzLEpNHS7AUIje3BCRaMgO+1XDh7YA1Q/OqvqkUGerPnhB9P47JMPAXjj7feIi0+os1ZEZCR5ebmUlpbidDpJ2Z9MfMUy8fEJ7N+fTEKbREpLS8nLzyUiMrLe7f8zjtxPJcU5bFn1LN37P42vX+2Ph5LibPKzN9Ct3+MNrjnvy1dZOOttAO6cMJPImMR6loBOPU7hwUn2qWhrls0hZc9mACJjEiuPrgHISttDREzbw5Z944vZTPtmLgBfPPMQbWLqvmhZdHgo2fkFlJaW4XT6sDctk8SKZRJjotiTmkHbuGhKS8vIOVBAdHioizNvmF++f43l86cAMPLe/1GQl87XU29i5L3/Iyg0utblCvLS2bv1V0bc8VmDa34yfQozPn0fgMlvfkxsPY9jgF59TmLqh/Zj6OdFP7Brx1YA4uLbVL5rA5C6f1+19b3+3U+8M3cpADMevI7EqLp/B0WHBpFTcJDSsjKcPj7szcypXCYxKpy9GTm0i46gtKyM3IJCokODXJx5wyib6pSNsqmPsqlO2Sib+iib6pRN82UD3pFPgxo1IeFdOXhgD4UFyfgFxJC+dx5d+o47bExk/EBSd88mNLIHGckLCI/pizGGqPiBbFr+BIkdL6O4KIODB/YQEtHN62p6Yo6N0ebo4bQ5erjL440xhMf0ISN5ATFtzyJ192wi4wcBtc/L0zU9Mcem0NDtBiguysLPP5LysmL2bvmIdp2vBuzt3r9jBjGJg8nPXofTGYxfQPVfhs1d86qrR3PV1aNdrmWMof+AgcyZPZOh51/El198xuCzzwXgzLPO4csvPqN3nxOZM3smA04e1CzZFB1MYeOyf9O598OV1+qpTUbyAiLjT8Hh49/gmoMvvpXBF9d8Zfra5GalEhYZR0lxEd9+/Aznj3wYgF4DL2Tel6/S/8wr2LZ+KUHB4UREH/7pZWP/PoSxfx/ici1jDKf36cGMBUu47KxBTJ89n2Gn2ufTDh10ItNnz2fA8V2YsWAJSX17uC2b/mffTP+zbwYgJ30Xn7w8guFj3yG6Td2fcrbu1y/o0nsoTr+ABtccMXIMI0aOadAymRlpREXHUlxcxLS3JjPmJvtCeEmDz+OTD6Zw3rDhrP79N0JCw4iNO/wJwE3nDeSm8wa6XMsYw+ndj2XGktVcNqg3HyxYxrB+3QEY2q87HyxYxoAuHZixZDVJPTopG2WjbFA2yqZ+yqY6ZaNs6vNXyAa8Ix9T14VsBl2woNqdWSlL2L7uFSyrnPj2f6Nd51Hs2jiVkPCuRCUMorysiM0rJ3AgZzNO3zD744+D7XeO92x+n5Td32KMDx173EZk3ACXJt7cNd1db/HXSY3+yakpm+LCDFYtGlvxEcIGH2cgvZPexekbzLqlD9Cp1334BcRQeGCf/dHVJbkEh3emc+9/4fDxq3NetWnumu6u5+lsdqx7jazUn7Esi4QOF5J4jH1xXcuy2L5mEllpv+Dj40+nXg/U2wR0Z823X6z+8dVpaalcPnwo+fn5OBwOgoKC+PrbHwgJDWXs9aN4/MlniYtPYPeunfzz7lvIzs7muO7H88xzL+Pn709RUSEP/PNO1q9bQ0REBM+9+F/aH9Whcv3HdWrrlmy2/P4MGfsX4h8YD4AxPvQ6zf40sKr7CWDNT3fSttNVh/1c17WfAcbcfXq17cjJ3M/jN5/EwYJcjHEQEBjC41PXEhgcxksPDeMf975FZEwin75xH6uWzKS8vJwzL7yJcy65C7Czmf7ybaz59Tv8AoK47r6pHN3VbqqM8P28Wr2UjGxOu/FB8g4cxOEwBAcGsOy9FwgLDuLv903k1QfG0iYmiu37Uhg9/iWy8vLp2bkjU8bdjr+fL4VFxVz/5Cus2rydyNAQpo2/i46J8ZXrDz7tskZnM/79kmrZ/G/KWNb/OoPwmKMAcDic3PjYEgCmP3chF455ndBI++d32oSzOfX8+w47Pzo/ez9vPnoKRQdzMQ4Hfv4h3PrU75XnRl80oPr1qdLTUrj6knM4kJ+HcTgICgrm81mLCQkJ5fYbruCRJ14iNj6BF58ez4/z52CVl3PplaMZOdr+OHrLsnjqsQf4+ccfCAgMZPyEl+l+Qm8Auq18p1q9/dl5nPrQy+QdLMRhDMEB/ix//l7CggK4eOIU/jv2UhKjwtmeksE1kz4kK7+AXkcnMvX2K/H3dVJYXMKYVz7m9x37iAwJ4r07r6Jj/KGGauDl9ykbZaNslI2yqULZKBtl03B/1WzAvfnUlU2DGzXSeO5qBkjjKRvX1NSocTd3NWrcraZGjTvV1KhxN3c1atytpicA7lTbEwB3cteTM3dTNq5RNu6hbFyjbFynbFyjbNxD2bjG27JpeVfVERERERERERFppdSoERERERERERHxEmrUiIiIiIiIiIh4CTVqRERERERERES8hBo1IiIiIiIiIiJeQo0aEREREREREREvoUaNiIiIiIiIiIiXUKNGRERERERERMRLqFEjIiIiIiIiIuIl1KgREREREREREfESxrKsWu8cdMGC2u+UP23x10mmsev4K2Tz0Lc3NnvN80s3tshs3n6xU7PW29ptcLPWg6bJZuq8On7huckI38+btd4PZ45r1nrQNNms2JTe7Nl0W/lOs9abe9XbzVoPlI2rlI3rlI1rlI17KBvXKBvXKRvXKBv3qCsbHVEjIiIiIiIiIuIl1KgREREREREREfESatSIiIiIiIiIiHgJNWpERERERERERLyEGjUiIiIiIiIiIl5CjRoRERERERERES+hRo2IiIiIiIiIiJdQo0ZERERERERExEuoUSMiIiIiIiIi4iUa3KjJSl3K8h9GsXzeVezZMr3a/eVlxWz87T8sn3cVqxbdTGFBcuV9e7ZMZ/m8q1j+wyiyUn/x2pqemGNTKMjfyapFt/DzrHPYu/XjWscVFiSzatHNLJ93FRt/+w/l5SVA3fPylpq/WQe4qWw7N5Zt57PyzGr3l1jlPF2+jxvLtnNv2S5SrJLK+z4rz+TGsu3cVLad5daBeufWlFzdTznpy/l94Q2sWDCazSsnYpWXAmBZFtvWvMzyeVexcsF15Odsqrfmtq1buPKyC+nV/Rimvv16reP27N7FiEvO57yzBnHPnTdTXFwMQHFREffceTPnnTWIEZecz949u+us11KzSd61gSdvG8jYIQHM/vS5WsetXzGP/4w9kX+POYEpT42mrOxQNh++cgcPjerMo9f3Yuem5fXW3LhzL4Nv/hdRZ13FpI/+V+u4HftSOWPsw/S88nauefRFikvsmkXFJVzz6Iv0vPJ2zhj7MDuTU+us11Kz2b51M/8Y8TcGHN+W96a8Wuu4X37+kauGD+ay80/jkQdupbT0UDbPPPEQF55zEpdfkMT6tb/XW3Pj3lTOGPcKESMf4qWvF9Q6bkdqJqf/azLH3/E0o176gOKKmkUlpYx66QOOv+NpTv/XZHamVt/fVSkbZdPUlI2yqUrZuEbZKJuqmjsbaJn5KBubu7JpUKPGssrYtmYS3fs/Te8z3iV97zwK8nYcNiZl9yycviH0HfwhiR0vZef6NwEoyNtB+t559E6aRvcBz7BtzUtYVpnX1fTEHJuK0zeMjsffQeIxI+oct3P9GyR2vJS+gz/E6RtC6q5ZQO3z8paaZZbF6+WpjHe05VXH0Sy0ctllFR02Zo6VSwg+vOnTkYtMBNOsNAB2WUUstHJ51dGB8Y52vFaeSpll1Tu/puLKfrKscjavnEiXvo/QJ2ka/oHxpO75DoDs1KUUHthDnzOnc2zPe9m2+sV6a4ZHRPDwvx/j2uvH1jnu+Wcn8I9rb+C7uYsJCwvni8/sRtL/ff4xYWHhfDd3Mf+49gaef3ZCretoydkEh0Zx1W2TOO+ye2sdU15ezpSnRzN23Ec8PmU10fFH8dN37wKw+pdvSdmzhQnvbeKae97g/Um31FszMiyEZ++4ljuuuKDOcf9+4wNuvXwYqz6aTERoMO/OnAfAuzPnEREazKqPJnPr5cP49+vVG8p/aMnZhEdEcP+/JjBqTO37tLy8nEcfvI2JL7zFZ9/8SJvE9nwzw34ML174Pbt2bOOrOb8w7vHnmTj+/nprRoYE8dzoi7jzgqQ6x42bPovbh57GmpcfICI4kGnzfgVg2rxfiAgOZM3LD3D70NMY9+GsWtehbJSNOygbZVOVsnGNslE2VTVnNtBy81E2Nndl06BGTX72BgKD2xIQnIjD4UtM28Fkpiw+bExWymLi2g8BILpNEjnpv2FZFpkpi4lpOxiHjx8BQW0IDG5LfvYGr6vpiTk2FT//SEIjumGMT61jLMsiJ3050W3sB3dc+yFkpiwCap+Xt9TcTCFt8CXB+OFrDKebMJYe0ZlcauVzlgkDYJAJ5XerAMuyWGod4HQThq9xkGB8aYMvmymsc25NyZX9VFqci3H4EhjSHoCImH5kJC8EIDNlMbHtzsMYQ2hkD0pL8ikuzKizZnR0DCf07I3T6ax1jGVZLF2ymHOHDAPg4r9fxtzv7ebQvO/ncPHfLwPg3CHDWPLzolaZTVhkHB27nYSP07fWMQdyM3A6/Uho3wWA7ieew28/fgHAysVfMfDcURhjOLb7yRTkZ5OdUfeRYXGR4Zx4XCd8fer+uVmwfC3Dk04GYOSQM/jmR/uPzMxFyxg55AwAhiedzPzla1plNlHRsfTo2QdnHdnkZGfi6+tHh47HAjBgUBJz53wDwPy5szn/4hEYY+jZux95uTmkpe6vs2ZceAj9OrXH16f2P4+WZbFg7RaGn3wCAFcn9eObX9cCMHPZOq5O6gfA8JNPYP6aLcpG2SgblA0oG1A2VSkb17T2bKDl5qNsbO7KpkGNmqKDafgFxFZ+7xcQS/HBtMPHFB4aYxxOfHxDKC3JofhgGv5HLFt0xLLeUNMTc2xOpSU5OH1DMA77xbtfQCxFhfY21jYvb6mZQSkx5lDTIRonGZRUH4M9xscYgvEhl3IyKKm8HSDGOMmgtNFza0pOv3Asq6yyuZeRvIDiQvuUluLCNPwDDz22/ANiKS5s/GMrOyuL0NCwymZOfEIbUlLsX5ApKftJSGhjb5vTSWhIGNlZWTWup7VnExIeQ1lZKTs2LgNg2cLPyUyzTwXLSt9HVGz7yrGRse3ITt/b6JoZOXlEhAThdNrNnLaxUexLtw+33JeeSbu4aACcTh/Cg4PIyMmreT2tPJuIyGhKy0pZt3olAHNnf03K/n0ApKYkE5+QWDk2LiGRtJS6nwC4IiOvgPCgQJwVjba2UeHsy7R/b+3LzKFtdDgATh8fwoICyMgrqHk9yqZyrLJpXsrGpmxsysY1ysambGxNlQ207nyUzZ/Ppva32kWkWRlj6Nr3EbavexWrrISI2H5gdL1vb2CMYey4j/j4v/dQWlJE937n4HDUfjSMNB9jDBNfeJPnJo6jpLiYkwedgcOhnxtvoGy8l7LxXsrGeykb76VsvJey+fMa1KjxDzz8XfziwjT8qrzLD4fe6fcPjMMqL6WsJB+nbzh+gYeOovhjWf8jlvWGmp6YY2Mk75hByi778LHu/Z/GLyCmzvFO33BKS/KxyksxDqe9jRVHtNQ2L2+oCXYHM9061IXMoJRofKuPoZQYfCmzLA5QRhgOovElvUoHM90qJdq4t0/Z0P0EEBrZgxMGTgYgO+1XDh7YA1Q/OqvqkUhVffjBND775EMA3nj7PeLiE+qsFxEZSV5eLqWlpTidTlL2JxNfsUx8fAL79yeT0CaR0tJS8vJziYiMrHE9LS2beV++ysJZbwNw54SZRMYk1rMEdOpxCg9Osk9FW7NsDil7NgMQGZNYeXQNQFbaHiJi2lZb/o0vZjPtm7kAfPHMQ7SJiaqzXnR4KNn5BZSWluF0+rA3LZPEimUSY6LYk5pB27hoSkvLyDlQQHR4aM3raWHZfDJ9CjM+fR+AyW9+TGw9j2GAXn1OYuqH9s/az4t+YNeOrQDExbepfNcGIHX/vhrX9/p3P/HO3KUAzHjwOhKjav4d9Ifo0CByCg5SWlaG08eHvZk5lcskRoWzNyOHdtERlJaVkVtQSHRoUM3rUTaV45RN4ygbZVOVsnGNslE2VXkqG2hZ+Sib5sumQe2skPCuHDywh8KCZMrLS0jfO4+o+IGHjYmMH0jq7tn2RJIXEB7TF2MMUfEDSd87j/KyYgoLkjl4YA8hEd28rqYn5tgYbY4eTu/Tp9D79CkuNQKMMYTH9CEj2b5Kduru2UTGDwJqn5c31AToTAD7KGG/VUKJZbHQyqW/CT5szAATwlwrF4DFVh49TRDGGPqbYBZauZRY5ey3SthHCZ0JqHfbG6Oh+wmguMg+tai8rJi9Wz4iocOFgL2f0vZ8h2VZ5GWtxekMxi8gutryV109mhlfz2HG13PqbdKAnU3/AQOZM3smAF9+8RmDzz4XgDPPOocvv/gMgDmzZzLg5EGtJpvBF9/K+DdXMP7NFS41aQBys+zT0EqKi/j242c44wL7Is29Bl7IT3Pex7Istq5bQlBwOBHRbaotP/bvQ/h56rP8PPXZeps0YGdzep8ezFiwBIDps+cz7FT7nNqhg05k+uz5AMxYsISkvj1aTTYjRo7h46/m8/FX81364w+QmWE3MYuLi5j21mQuuWI0AEmDz+ObLz/BsixWrVxGSGgYsXHV13nTeQNZ+szdLH3m7nr/+ENFNt2PZcaS1QB8sGAZw/p1B2Bov+58sMA+RW7GktUk9eikbFA2ykbZKBubslE2rvgrZQMtKx9l03zZmLounjPoggXV7sxKWcL2da9gWeXEt/8b7TqPYtfGqYSEdyUqYRDlZUVsXjmBAzmbcfqG0aXvIwQE2y+E9mx+n5Td32KMDx173EZk3ACXNrK5a7q73uKvk2p/NLiopmyKCzNYtWgsZaUFgMHHGUjvpHdx+gazbukDdOp1H34BMRQe2Mem5Y9RWpJLcHhnOvf+Fw4fvzrnVRt31nzo2xur1Vtm5fNWeRrlwNkmjBGOaD4oT6ezCWCACaHYKueF8v1so4gQHNzvaEOC8QPgk/IMvrdy8QGud8TR74gfMoDzSzd6NJsd614jK/VnLMsiocOFJB5jX8zXsiy2r5lEVtov+Pj406nXA4c1Ad9+sVO17UhLS+Xy4UPJz8/H4XAQFBTE19/+QEhoKGOvH8XjTz5LXHwCu3ft5J9330J2djbHdT+eZ557GT9/f4qKCnngn3eyft0aIiIieO7F/9L+qA4AbO02uFq9lpDN1HnVf+HlZO7n8ZtP4mBBLsY4CAgM4fGpawkMDuOlh4bxj3vfIjImkU/fuI9VS2ZSXl7OmRfexDmX3AXY2Ux/+TbW/PodfgFBXHffVI7u2q9y/SN8P6+2HSkZ2Zx244PkHTiIw2EIDgxg2XsvEBYcxN/vm8irD4ylTUwU2/elMHr8S2Tl5dOzc0emjLsdfz9fCouKuf7JV1i1eTuRoSFMG38XHRPjAfjhzHHV6rWEbFZsSq+WTXpaCldfcg4H8vMwDgdBQcF8PmsxISGh3H7DFTzyxEvExifw4tPj+XH+HKzyci69cjQjR98E2Nk89dgD/PzjDwQEBjJ+wst0P6F35fq7rXyn2nbsz87j1IdeJu9gIQ5jCA7wZ/nz9xIWFMDFE6fw37GXkhgVzvaUDK6Z9CFZ+QX0OjqRqbdfib+vk8LiEsa88jG/79hHZEgQ7915FR3j7abq3KverlZP2SibxlA2ykbZNJyyUTbelg24Nx9l0zKzaXCjRhrPXY2a1qamRo27uatR4241NWrcqaZGjbu5q1HjbjU1atyppkaNu7nrCYC71fQEwJ1qewLgTsrGNcrGdcrGNcrGPZSNa5SN65SNa5SNe9SVja7kIyIiIiIiIiLiJdSoERERERERERHxEmrUiIiIiIiIiIh4CTVqRERERERERES8hBo1IiIiIiIiIiJeQo0aEREREREREREvoUaNiIiIiIiIiIiXUKNGRERERERERMRLqFEjIiIiIiIiIuIl1KgREREREREREfESxrIsT2+DiIiIiIiIiIigI2pERERERERERLyGGjUiIiIiIiIiIl5CjRoRERERERERES+hRo2IiIiIiIiIiJdQo0ZERERERERExEuoUSMiIiIiIiIi4iX+P1uw1Oci7wNpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x360 with 11 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-import necessary libraries after reset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters for the gridworld\n",
    "grid_size = (3, 3)\n",
    "rewards = np.full(grid_size, -1.0)  # Reward = -1 for all states\n",
    "rewards[-1, -1] = 10.0  # Goal state reward\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros(grid_size)\n",
    "\n",
    "# Policy (greedy towards the goal)\n",
    "policy = {\n",
    "    (0, 0): (0, 1), (0, 1): (0, 1), (0, 2): (1, 0),\n",
    "    (1, 0): (0, 1), (1, 1): (0, 1), (1, 2): (1, 0),\n",
    "    (2, 0): (0, 1), (2, 1): (0, 1), (2, 2): None  # Goal state has no action\n",
    "}\n",
    "\n",
    "# Iterative application of the Bellman Equation\n",
    "iterations = 10\n",
    "values_history = []\n",
    "\n",
    "for _ in range(iterations):\n",
    "    new_V = np.copy(V)\n",
    "    values_history.append(np.copy(V))\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            if (i, j) == (2, 2):  # Skip the goal state\n",
    "                continue\n",
    "            action = policy[(i, j)]\n",
    "            next_state = (i + action[0], j + action[1])\n",
    "            new_V[i, j] = rewards[i, j] + gamma * V[next_state]\n",
    "    V = new_V\n",
    "\n",
    "values_history.append(np.copy(V))  # Final value function\n",
    "\n",
    "# Visualization of the gridworld\n",
    "fig, axes = plt.subplots(1, iterations + 1, figsize=(20, 5))\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.imshow(values_history[idx], cmap='coolwarm', interpolation='nearest')\n",
    "    ax.set_title(f\"Iteration {idx}\")\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            ax.text(j, i, f\"{values_history[idx][i, j]:.2f}\", ha='center', va='center', color='black')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100625e",
   "metadata": {},
   "source": [
    "# Stochastic Iterative Approach to Gridworld\n",
    "\n",
    "In a stochastic environment, the iterative approach to compute the value function involves accounting for probabilistic transitions between states. This changes how the Bellman Equation is applied compared to the deterministic case.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Changes in Stochastic Environments\n",
    "\n",
    "1. **Stochastic Transitions**:\n",
    "   - An action no longer leads to a single deterministic next state.\n",
    "   - Instead, it leads to multiple possible next states, each with a probability of occurrence, $P(s' \\mid s, a)$.\n",
    "\n",
    "2. **Updated Bellman Equation**:\n",
    "   - For a state $s$ under a policy $\\pi(s)$, the Bellman Equation becomes:\n",
    "     $$\n",
    "     V(s) = R(s, \\pi(s)) + \\gamma \\sum_{s'} P(s' \\mid s, \\pi(s)) V(s')\n",
    "     $$\n",
    "   - Here, the value of a state $s$ is the immediate reward $R(s, \\pi(s))$ plus the expected discounted value of all possible next states $s'$, weighted by their transition probabilities.\n",
    "\n",
    "3. **Policy**:\n",
    "   - The policy $\\pi(s)$ can remain deterministic (always choose a specific action in state $s$) or become stochastic (choose actions probabilistically). In this example, we use a deterministic policy.\n",
    "\n",
    "---\n",
    "\n",
    "## Iterative Process for Value Function Calculation\n",
    "\n",
    "### Inputs:\n",
    "- **Gridworld**: A grid of states where each state has possible actions and rewards.\n",
    "- **Policy**: A mapping from states to actions, $\\pi(s)$.\n",
    "- **Rewards**: $R(s, a)$ for each state-action pair.\n",
    "- **Transition Probabilities**: $P(s' \\mid s, a)$ for all state-action pairs.\n",
    "- **Discount Factor**: $\\gamma$ (value between $0$ and $1$).\n",
    "- **Maximum Iterations**: The number of iterations to compute values.\n",
    "\n",
    "---\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Initialize Value Function**:\n",
    "   - Set $V(s) = 0$ for all states $s$.\n",
    "\n",
    "2. **Iterative Updates**:\n",
    "   - For each state $s$ (excluding terminal states):\n",
    "     - Determine the action $a = \\pi(s)$ from the policy.\n",
    "     - Compute the expected value of transitioning to all possible next states $s'$:\n",
    "       $$\n",
    "       V_{\\text{new}}(s) = R(s, a) + \\gamma \\sum_{s'} P(s' \\mid s, a) V(s')\n",
    "       $$\n",
    "     - Update $V_{\\text{new}}(s)$ for all states $s$.\n",
    "\n",
    "3. **Update Values**:\n",
    "   - Set $V(s) = V_{\\text{new}}(s)$ for all states.\n",
    "\n",
    "4. **Repeat Until Convergence**:\n",
    "   - Stop when the change in value for all states is below a small threshold $\\epsilon$ or after a maximum number of iterations.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Stochastic Gridworld\n",
    "\n",
    "#### Parameters:\n",
    "- **Grid Size**: $3 \\times 3$\n",
    "- **Rewards**: $R(s, a) = -1$ for all states except the goal state $(2, 2)$, where $R(s, a) = +10$.\n",
    "- **Policy**: Move toward the goal deterministically.\n",
    "- **Transition Probabilities**:\n",
    "  - Example for action \"right\":\n",
    "    - $P(s' \\mid s, \\text{right}) = 0.8$ (intended move).\n",
    "    - $P(s' \\mid s, \\text{down}) = 0.1$ (slip down).\n",
    "    - $P(s' \\mid s, \\text{stay}) = 0.1$ (fail to move).\n",
    "\n",
    "#### Bellman Equation Example:\n",
    "For state $(0, 0)$ (starting state) with action \"right\":\n",
    "$$\n",
    "V(0, 0) = R(0, 0, \\text{right}) + \\gamma \\left[\n",
    "0.8 V(0, 1) + 0.1 V(1, 0) + 0.1 V(0, 0)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Convergence:\n",
    "After multiple iterations, the value function stabilizes. The final value for each state represents the expected cumulative discounted reward under the given policy, accounting for stochastic transitions.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insights:\n",
    "\n",
    "1. **Handling Stochasticity**:\n",
    "   - The value function accounts for uncertainty by weighting possible outcomes with their probabilities.\n",
    "\n",
    "2. **Propagation of Values**:\n",
    "   - Values propagate backward from the goal state through iterative updates, incorporating transition probabilities.\n",
    "\n",
    "3. **Convergence**:\n",
    "   - The process converges when the values of all states change by less than a small threshold $\\epsilon$ or after a set number of iterations.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizing Iterations:\n",
    "The value function evolves over time as values propagate backward from the goal state. Each iteration refines the estimate of $V(s)$ for all states.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Value Function:\n",
    "After convergence, the value function might look like this:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-3.44 & -2.71 & -1.90 \\\\\n",
    "-2.71 & -1.90 & -1.00 \\\\\n",
    "-1.90 & -1.00 & 10.00\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "This reflects the expected cumulative discounted rewards from each state under the policy, accounting for stochastic transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "791586fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]]), array([[-1., -1., -1.],\n",
      "       [-1., -1., -1.],\n",
      "       [-1., -1.,  0.]]), array([[-1.9 , -1.9 , -1.9 ],\n",
      "       [-1.9 , -1.9 , -1.18],\n",
      "       [-1.9 , -1.18,  0.  ]]), array([[-2.71  , -2.71  , -2.1916],\n",
      "       [-2.71  , -2.1268, -1.2124],\n",
      "       [-2.1916, -1.2124,  0.    ]]), array([[-3.439   , -3.013264, -2.267416],\n",
      "       [-2.97244 , -2.173456, -1.218232],\n",
      "       [-2.267416, -1.218232,  0.      ]]), array([[-3.74657968, -3.09934432, -2.28526192],\n",
      "       [-3.03647536, -2.18237896, -1.21928176],\n",
      "       [-2.28526192, -1.21928176,  0.        ]]), array([[-3.84200286, -3.12074368, -2.28923001],\n",
      "       [-3.05026921, -2.18403233, -1.21947072],\n",
      "       [-2.28923001, -1.21947072,  0.        ]]), array([[-3.86723993, -3.12567545, -2.29008032],\n",
      "       [-3.05305821, -2.18433419, -1.21950473],\n",
      "       [-2.29008032, -1.21950473,  0.        ]]), array([[-3.87331316, -3.1267587 , -2.29025786],\n",
      "       [-3.05360308, -2.18438891, -1.21951085],\n",
      "       [-2.29025786, -1.21951085,  0.        ]]), array([[-3.87468872, -3.12698895, -2.29029423],\n",
      "       [-3.0537075 , -2.18439879, -1.21951195],\n",
      "       [-2.29029423, -1.21951195,  0.        ]]), array([[-3.8749877 , -3.12703674, -2.29030157],\n",
      "       [-3.05372729, -2.18440057, -1.21951215],\n",
      "       [-2.29030157, -1.21951215,  0.        ]])]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 11 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-importing necessary libraries after reset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters for the stochastic gridworld\n",
    "grid_size = (3, 3)\n",
    "rewards = np.full(grid_size, -1.0)  # Reward = -1 for all states\n",
    "rewards[-1, -1] = 10.0  # Goal state reward\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Transition probabilities\n",
    "# Example: Move right with P=0.8, slip down with P=0.1, stay in place with P=0.1\n",
    "transition_probs = {\n",
    "    \"right\": [(0, 1, 0.8), (1, 0, 0.1), (0, 0, 0.1)],\n",
    "    \"down\": [(1, 0, 0.8), (0, 1, 0.1), (0, 0, 0.1)],\n",
    "    \"left\": [(0, -1, 0.8), (1, 0, 0.1), (0, 0, 0.1)],\n",
    "    \"up\": [(-1, 0, 0.8), (0, 1, 0.1), (0, 0, 0.1)],\n",
    "}\n",
    "\n",
    "# Policy (deterministic for simplicity)\n",
    "policy = {\n",
    "    (0, 0): \"right\", (0, 1): \"right\", (0, 2): \"down\",\n",
    "    (1, 0): \"right\", (1, 1): \"right\", (1, 2): \"down\",\n",
    "    (2, 0): \"right\", (2, 1): \"right\", (2, 2): None  # Goal state has no action\n",
    "}\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros(grid_size)\n",
    "\n",
    "# Helper function to get valid next states\n",
    "def get_next_states(state, action):\n",
    "    i, j = state\n",
    "    next_states = []\n",
    "    for di, dj, prob in transition_probs[action]:\n",
    "        ni, nj = i + di, j + dj\n",
    "        if 0 <= ni < grid_size[0] and 0 <= nj < grid_size[1]:  # Valid state\n",
    "            next_states.append(((ni, nj), prob))\n",
    "        else:  # If next state is out of bounds, remain in the same state\n",
    "            next_states.append(((i, j), prob))\n",
    "    return next_states\n",
    "\n",
    "# Iterative application of Bellman Equation\n",
    "iterations = 10\n",
    "values_history = []\n",
    "\n",
    "for _ in range(iterations):\n",
    "    new_V = np.copy(V)\n",
    "    values_history.append(np.copy(V))\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            state = (i, j)\n",
    "            if state == (2, 2):  # Skip the goal state\n",
    "                continue\n",
    "            action = policy[state]\n",
    "            value = 0\n",
    "            for (next_state, prob) in get_next_states(state, action):\n",
    "                value += prob * (rewards[state] + gamma * V[next_state])\n",
    "            new_V[state] = value\n",
    "    V = new_V\n",
    "\n",
    "values_history.append(np.copy(V))  # Final value function\n",
    "\n",
    "print(values_history)\n",
    "\n",
    "# Visualization of the gridworld\n",
    "fig, axes = plt.subplots(1, iterations + 1, figsize=(20, 5))\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.imshow(values_history[idx], cmap='coolwarm', interpolation='nearest')\n",
    "    ax.set_title(f\"Iteration {idx}\")\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            ax.text(j, i, f\"{values_history[idx][i, j]:.2f}\", ha='center', va='center', color='black')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640a445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f86324ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 11 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters for the stochastic gridworld\n",
    "grid_size = (3, 3)\n",
    "rewards = np.full(grid_size, -1.0)  # Reward = -1 for all states\n",
    "rewards[-1, -1] = 10.0  # Goal state reward\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Transition probabilities\n",
    "# Example: Move right with P=0.8, slip down with P=0.1, stay in place with P=0.1\n",
    "transition_probs = {\n",
    "    \"right\": [(0, 1, 0.8), (1, 0, 0.1), (0, 0, 0.1)],\n",
    "    \"down\": [(1, 0, 0.8), (0, 1, 0.1), (0, 0, 0.1)],\n",
    "    \"left\": [(0, -1, 0.8), (1, 0, 0.1), (0, 0, 0.1)],\n",
    "    \"up\": [(-1, 0, 0.8), (0, 1, 0.1), (0, 0, 0.1)],\n",
    "}\n",
    "\n",
    "# Policy (deterministic for simplicity)\n",
    "policy = {\n",
    "    (0, 0): \"right\", (0, 1): \"right\", (0, 2): \"down\",\n",
    "    (1, 0): \"right\", (1, 1): \"right\", (1, 2): \"down\",\n",
    "    (2, 0): \"right\", (2, 1): \"right\", (2, 2): None  # Goal state has no action\n",
    "}\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros(grid_size)\n",
    "\n",
    "# Precompute distances from the goal state\n",
    "goal_state = (2, 2)\n",
    "state_distances = {\n",
    "    (i, j): abs(i - goal_state[0]) + abs(j - goal_state[1])\n",
    "    for i in range(grid_size[0]) for j in range(grid_size[1])\n",
    "}\n",
    "\n",
    "# Sort states by distance from the goal\n",
    "sorted_states = sorted(state_distances.keys(), key=lambda s: state_distances[s])\n",
    "\n",
    "# Iterative application of Bellman Equation with prioritized updates\n",
    "iterations = 10\n",
    "values_history = []\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    new_V = np.copy(V)\n",
    "    values_history.append(np.copy(V))\n",
    "    for state in sorted_states:\n",
    "        if state == goal_state:  # Skip the goal state\n",
    "            continue\n",
    "        i, j = state\n",
    "        action = policy[state]\n",
    "        value = 0\n",
    "        for (next_state, prob) in get_next_states(state, action):\n",
    "            value += prob * (rewards[state] + gamma * V[next_state])\n",
    "        new_V[state] = value\n",
    "    V = new_V\n",
    "\n",
    "values_history.append(np.copy(V))  # Add final value function\n",
    "\n",
    "# Visualization of prioritized updates\n",
    "fig, axes = plt.subplots(1, len(values_history), figsize=(20, 5))\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.imshow(values_history[idx], cmap='coolwarm', interpolation='nearest')\n",
    "    ax.set_title(f\"Iteration {idx}\")\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            ax.text(j, i, f\"{values_history[idx][i, j]:.2f}\", ha='center', va='center', color='black')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31835f54",
   "metadata": {},
   "source": [
    "# What has changed in the above code:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Sorting States by Distance**\n",
    "- States are sorted in ascending order of their distance to the goal state.\n",
    "- This ensures that states closer to the goal are updated first in every iteration.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Prioritized Updates**\n",
    "- Instead of updating states row-by-row and column-by-column, states are updated in the order of their sorted distances.\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits of the Changes\n",
    "1. **Explicit Update Order**:\n",
    " - States near the goal are updated first in each iteration, reflecting their dependency on the goal state's value.\n",
    "\n",
    "2. **Faster Convergence for Near States**:\n",
    " - States adjacent to the goal stabilize faster, as they directly rely on the fixed value of the goal state.\n",
    "\n",
    "3. **Improved Clarity**:\n",
    " - The update process aligns with the natural propagation of values backward from the goal, making the propagation visually and conceptually clear.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualization of the Changes\n",
    "The visualization now shows how states near the goal state update first, propagating values outward to states further away. This refinement ensures a more structured and logical order of updates, aligning with the principles of backward propagation.\n",
    "\n",
    "---\n",
    "\n",
    "## Unchanged Aspects\n",
    "- The **policy evaluation process** using the Bellman Equation remains the same.\n",
    "- The transition probabilities and reward structure of the stochastic gridworld are unchanged.\n",
    "- The stopping condition for convergence (number of iterations) is retained.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54b9cd",
   "metadata": {},
   "source": [
    "# Implementing Policy Improvement and Policy Iteration (Alternating between Policy Evaluation and Policy Improvement):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "216cf9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['right', 'right', 'right'],\n",
       "        ['right', 'right', 'right'],\n",
       "        ['right', 'right', 'Goal']], dtype=object),\n",
       " array([['down', 'right', 'down'],\n",
       "        ['down', 'down', 'down'],\n",
       "        ['right', 'right', 'Goal']], dtype=object),\n",
       " array([[-4.03318298, -3.29653919, -2.48041508],\n",
       "        [-3.2965387 , -2.37723871, -1.33401262],\n",
       "        [-2.48041491, -1.33401262,  0.        ]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters for the gridworld\n",
    "grid_size = (3, 3)\n",
    "rewards = np.full(grid_size, -1.0)  # Reward = -1 for all states\n",
    "rewards[-1, -1] = 10.0  # Goal state reward\n",
    "gamma = 0.9  # Discount factor\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "# Transition probabilities for stochastic gridworld\n",
    "transition_probs = {\n",
    "    \"up\": [(-1, 0, 0.8), (0, -1, 0.1), (0, 1, 0.1)],\n",
    "    \"down\": [(1, 0, 0.8), (0, -1, 0.1), (0, 1, 0.1)],\n",
    "    \"left\": [(0, -1, 0.8), (-1, 0, 0.1), (1, 0, 0.1)],\n",
    "    \"right\": [(0, 1, 0.8), (-1, 0, 0.1), (1, 0, 0.1)],\n",
    "}\n",
    "\n",
    "# Initialize a suboptimal policy: Always move right\n",
    "policy = {(i, j): \"right\" for i in range(grid_size[0]) for j in range(grid_size[1])}\n",
    "policy[(2, 2)] = None  # No action for the goal state\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros(grid_size)\n",
    "\n",
    "\n",
    "# Helper function to get valid next states\n",
    "def get_next_states(state, action):\n",
    "    i, j = state\n",
    "    next_states = []\n",
    "    for di, dj, prob in transition_probs[action]:\n",
    "        ni, nj = i + di, j + dj\n",
    "        if 0 <= ni < grid_size[0] and 0 <= nj < grid_size[1]:  # Valid state\n",
    "            next_states.append(((ni, nj), prob))\n",
    "        else:  # Out-of-bounds, remain in the same state\n",
    "            next_states.append(((i, j), prob))\n",
    "    return next_states\n",
    "\n",
    "\n",
    "# Policy evaluation: Iteratively calculate the value function for a given policy\n",
    "def policy_evaluation(policy, V, iterations=100, theta=1e-6):\n",
    "    for _ in range(iterations):\n",
    "        delta = 0\n",
    "        new_V = np.copy(V)\n",
    "        for i in range(grid_size[0]):\n",
    "            for j in range(grid_size[1]):\n",
    "                state = (i, j)\n",
    "                if policy[state] is None:  # Skip terminal state\n",
    "                    continue\n",
    "                action = policy[state]\n",
    "                value = 0\n",
    "                for next_state, prob in get_next_states(state, action):\n",
    "                    value += prob * (rewards[state] + gamma * V[next_state])\n",
    "                new_V[state] = value\n",
    "                delta = max(delta, abs(new_V[state] - V[state]))\n",
    "        V = new_V\n",
    "        if delta < theta:  # Convergence\n",
    "            break\n",
    "    return V\n",
    "\n",
    "\n",
    "# Policy improvement: Update the policy based on the current value function\n",
    "def policy_improvement(V):\n",
    "    new_policy = {}\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            state = (i, j)\n",
    "            if state == (2, 2):  # Skip terminal state\n",
    "                new_policy[state] = None\n",
    "                continue\n",
    "            action_values = {}\n",
    "            for action in actions:\n",
    "                value = 0\n",
    "                for next_state, prob in get_next_states(state, action):\n",
    "                    value += prob * (rewards[state] + gamma * V[next_state])\n",
    "                action_values[action] = value\n",
    "            new_policy[state] = max(action_values, key=action_values.get)  # Best action\n",
    "    return new_policy\n",
    "\n",
    "\n",
    "# Policy iteration: Alternate between evaluation and improvement until convergence\n",
    "def policy_iteration(policy, V, max_iterations=100):\n",
    "    policy_history = [policy.copy()]\n",
    "    for _ in range(max_iterations):\n",
    "        # Policy evaluation\n",
    "        V = policy_evaluation(policy, V)\n",
    "        # Policy improvement\n",
    "        new_policy = policy_improvement(V)\n",
    "        policy_history.append(new_policy.copy())\n",
    "        if new_policy == policy:  # Convergence\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy, V, policy_history\n",
    "\n",
    "\n",
    "# Perform policy iteration\n",
    "final_policy, final_values, policy_history = policy_iteration(policy, V)\n",
    "\n",
    "# Visualize the results\n",
    "def visualize_policy(policy):\n",
    "    grid = np.zeros(grid_size, dtype=object)\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            grid[i, j] = policy[(i, j)] if policy[(i, j)] is not None else \"Goal\"\n",
    "    return grid\n",
    "\n",
    "\n",
    "initial_policy_visual = visualize_policy(policy)\n",
    "final_policy_visual = visualize_policy(final_policy)\n",
    "\n",
    "(initial_policy_visual, final_policy_visual, final_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb9f674",
   "metadata": {},
   "source": [
    "# L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b0fc5",
   "metadata": {},
   "source": [
    "# Policy Gradients: An Introduction\n",
    "\n",
    "**Policy Gradient (PG)** methods are a class of reinforcement learning (RL) algorithms that directly optimize the policy, represented as a parameterized function, by following the gradient of expected reward. Unlike **value-based methods** (e.g., Policy Iteration, Value Iteration), which indirectly derive policies from value functions, Policy Gradient methods work directly in the space of policies.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts of Policy Gradients\n",
    "\n",
    "1. **Policy Parameterization**:\n",
    "   - Policies are represented as a parameterized function $\\pi_\\theta(a \\mid s)$, where $\\theta$ is a vector of parameters (e.g., weights of a neural network).\n",
    "   - $\\pi_\\theta(a \\mid s)$ gives the probability of taking action $a$ in state $s$ under the policy.\n",
    "\n",
    "2. **Objective Function**:\n",
    "   - The goal is to maximize the expected cumulative reward, often expressed as:\n",
    "     $$\n",
    "     J(\\theta) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\right]\n",
    "     $$\n",
    "   - $J(\\theta)$ depends on the policy parameters $\\theta$.\n",
    "\n",
    "3. **Policy Gradient Theorem**:\n",
    "   - The gradient of the objective function can be expressed as:\n",
    "     $$\n",
    "     \\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi \\left[ \\nabla_\\theta \\log \\pi_\\theta(a \\mid s) \\cdot G_t \\right]\n",
    "     $$\n",
    "   - $G_t$ is the cumulative discounted reward from time $t$:\n",
    "     $$\n",
    "     G_t = \\sum_{k=t}^\\infty \\gamma^{k-t} r_k\n",
    "     $$\n",
    "\n",
    "4. **Update Rule**:\n",
    "   - Update the policy parameters using gradient ascent:\n",
    "     $$\n",
    "     \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)\n",
    "     $$\n",
    "   - $\\alpha$ is the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Policy Gradients?\n",
    "\n",
    "1. **Continuous Action Spaces**:\n",
    "   - Policy Gradient methods can handle continuous action spaces, unlike value-based methods.\n",
    "\n",
    "2. **Stochastic Policies**:\n",
    "   - PG methods can represent stochastic policies, which are essential for environments with inherent randomness.\n",
    "\n",
    "3. **Direct Optimization**:\n",
    "   - Avoids computing and storing value functions explicitly, focusing instead on directly optimizing the policy.\n",
    "\n",
    "---\n",
    "\n",
    "## The Policy Gradient Theorem\n",
    "\n",
    "### Objective Function\n",
    "The objective is to maximize the expected return:\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r_t \\right]\n",
    "$$\n",
    "\n",
    "### Gradient of the Objective\n",
    "Using the **log-derivative trick** and the **Markov property**, the gradient can be expressed as:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi \\left[ \\nabla_\\theta \\log \\pi_\\theta(a \\mid s) \\cdot G_t \\right]\n",
    "$$\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta(a \\mid s)$: Measures how the policy's probability of selecting an action changes with respect to $\\theta$.\n",
    "- $G_t$: The cumulative discounted reward (return) used as a surrogate measure for how good the action was.\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm: Vanilla Policy Gradient (REINFORCE)\n",
    "\n",
    "1. **Initialize**:\n",
    "   - Policy parameters $\\theta$ (e.g., weights of a neural network).\n",
    "\n",
    "2. **Collect Trajectories**:\n",
    "   - Simulate the environment using the current policy $\\pi_\\theta$ and collect states, actions, and rewards.\n",
    "\n",
    "3. **Compute Returns**:\n",
    "   - For each time step $t$, compute the return $G_t$:\n",
    "     $$\n",
    "     G_t = \\sum_{k=t}^\\infty \\gamma^{k-t} r_k\n",
    "     $$\n",
    "\n",
    "4. **Update Policy**:\n",
    "   - Update the policy parameters using the policy gradient:\n",
    "     $$\n",
    "     \\theta \\leftarrow \\theta + \\alpha \\mathbb{E}_\\pi \\left[ \\nabla_\\theta \\log \\pi_\\theta(a \\mid s) \\cdot G_t \\right]\n",
    "     $$\n",
    "\n",
    "5. **Repeat**:\n",
    "   - Iterate until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Gridworld Example with Policy Gradients\n",
    "\n",
    "### Setup\n",
    "1. **Grid**: A $4 \\times 4$ gridworld.\n",
    "2. **States**: $(i, j)$ indices of the grid.\n",
    "3. **Actions**: Up, Down, Left, Right.\n",
    "4. **Policy**:\n",
    "   - Parameterized as a table of probabilities for each action in each state.\n",
    "5. **Objective**:\n",
    "   - Maximize the expected reward from the start state to the goal state.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "627f132c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Policy Parameters (theta):\n",
      "[[[-44.4877364  -44.35672421 -44.38449961 -44.51986631]\n",
      "  [-21.88427872 -21.7969692  -21.7618246  -21.79046427]\n",
      "  [-12.13378238 -11.37829361 -12.22842436 -12.13994966]\n",
      "  [ -6.85612009  -6.30260557  -7.21170274  -6.89938609]]\n",
      "\n",
      " [[-21.45595724 -20.73208798 -21.57113277 -21.24316204]\n",
      "  [-14.12756111 -13.24371938 -14.18233014 -13.49046189]\n",
      "  [ -8.74667091  -7.95009768  -9.23946209  -7.94598553]\n",
      "  [ -5.15643129  -3.51932026  -5.23104671  -5.01444583]]\n",
      "\n",
      " [[-11.606646   -10.74273384 -10.80490805 -10.41220689]\n",
      "  [ -8.34164563  -7.54934605  -8.28616463  -7.3092064 ]\n",
      "  [ -4.33934556  -3.26718876  -4.73614812  -3.08150986]\n",
      "  [ -2.74254316   0.30424971  -2.59721999  -1.98875294]]\n",
      "\n",
      " [[ -6.90139982  -7.00776272  -7.05012821  -5.8047047 ]\n",
      "  [ -4.15633533  -4.13538469  -4.16521136  -2.79993556]\n",
      "  [ -2.73847157  -1.98761178  -2.42485043   0.43193795]\n",
      "  [  0.11143631   0.13129028   0.80610132   0.70293927]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Gridworld Parameters\n",
    "grid_size = (4, 4)\n",
    "n_actions = 4  # up, down, left, right\n",
    "gamma = 0.9  # Discount factor\n",
    "alpha = 0.01  # Learning rate\n",
    "n_episodes = 1000  # Number of training episodes\n",
    "\n",
    "# Initialize random policy parameters (theta)\n",
    "theta = np.random.rand(grid_size[0], grid_size[1], n_actions)\n",
    "\n",
    "# Action mapping\n",
    "action_map = {\n",
    "    0: (-1, 0),  # up\n",
    "    1: (1, 0),   # down\n",
    "    2: (0, -1),  # left\n",
    "    3: (0, 1)    # right\n",
    "}\n",
    "\n",
    "# Compute softmax probabilities for actions\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "# Choose action based on policy\n",
    "def choose_action(state, theta):\n",
    "    i, j = state\n",
    "    probs = softmax(theta[i, j])\n",
    "    return np.random.choice(n_actions, p=probs)\n",
    "\n",
    "# Step function\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "    di, dj = action_map[action]\n",
    "    ni, nj = i + di, j + dj\n",
    "    if 0 <= ni < grid_size[0] and 0 <= nj < grid_size[1]:\n",
    "        next_state = (ni, nj)\n",
    "    else:\n",
    "        next_state = state\n",
    "    reward = -1 if next_state != (3, 3) else 0\n",
    "    done = next_state == (3, 3)\n",
    "    return next_state, reward, done\n",
    "\n",
    "# Policy Gradient Training\n",
    "for episode in range(n_episodes):\n",
    "    state = (0, 0)\n",
    "    trajectory = []\n",
    "    rewards = []\n",
    "    \n",
    "    # Generate an episode\n",
    "    while True:\n",
    "        action = choose_action(state, theta)\n",
    "        next_state, reward, done = step(state, action)\n",
    "        trajectory.append((state, action))\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Compute returns G_t\n",
    "    G = 0\n",
    "    returns = []\n",
    "    for reward in reversed(rewards):\n",
    "        G = reward + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    # Update policy parameters\n",
    "    for (state, action), G_t in zip(trajectory, returns):\n",
    "        i, j = state\n",
    "        probs = softmax(theta[i, j])\n",
    "        theta[i, j, action] += alpha * (1 - probs[action]) * G_t\n",
    "\n",
    "print(\"Trained Policy Parameters (theta):\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1c1fb",
   "metadata": {},
   "source": [
    "# Upper Confidence Bound (UCB) in Reinforcement Learning\n",
    "\n",
    "---\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "The **Upper Confidence Bound (UCB)** is a popular method for balancing exploration and exploitation in **bandit problems** and reinforcement learning. UCB ensures that an agent systematically explores less-visited actions while exploiting the ones that appear to yield high rewards.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Idea of UCB**\n",
    "\n",
    "The UCB algorithm selects actions based on an **upper confidence bound** for the estimated value of each action. This bound reflects both:\n",
    "1. **Exploitation**: Favoring actions with higher estimated rewards.\n",
    "2. **Exploration**: Favoring actions that have been tried fewer times to reduce uncertainty.\n",
    "\n",
    "The balance is achieved by adding a confidence term to the estimated reward:\n",
    "$$\n",
    "A_t = \\arg\\max_a \\left[ Q(a) + c \\sqrt{\\frac{\\ln t}{N(a)}} \\right]\n",
    "$$\n",
    "Where:\n",
    "- $ Q(a) $: Current estimate of the value of action $a$.\n",
    "- $ t $: Current time step.\n",
    "- $ N(a) $: Number of times action $a$ has been selected.\n",
    "- $ c $: Exploration parameter that controls the degree of exploration.\n",
    "\n",
    "---\n",
    "\n",
    "## **UCB in Reinforcement Learning**\n",
    "\n",
    "In the context of **Reinforcement Learning (RL)**, UCB is often applied to:\n",
    "- Multi-armed bandits,\n",
    "- Tabular environments,\n",
    "- Exploration strategies in large state-action spaces.\n",
    "\n",
    "It guides the agent to explore actions with high uncertainty while exploiting high-reward actions.\n",
    "\n",
    "---\n",
    "\n",
    "## **How UCB Works**\n",
    "\n",
    "### **1. Initialization**\n",
    "- Initialize $ Q(a) $ (action value estimates) to some default value, typically $ 0 $.\n",
    "- Set $ N(a) = 0 $ for all actions.\n",
    "\n",
    "### **2. Action Selection**\n",
    "- At each time step $ t $, select the action $ a $ that maximizes:\n",
    "  $$\n",
    "  Q(a) + c \\sqrt{\\frac{\\ln t}{N(a)}}\n",
    "  $$\n",
    "- If $ N(a) = 0 $, the confidence term becomes infinite, ensuring that untried actions are always explored.\n",
    "\n",
    "### **3. Reward Update**\n",
    "- After selecting an action, observe the reward $ r $.\n",
    "- Update the action value estimate $ Q(a) $ using an incremental mean:\n",
    "  $$\n",
    "  Q(a) \\leftarrow Q(a) + \\frac{1}{N(a)} \\left( r - Q(a) \\right)\n",
    "  $$\n",
    "\n",
    "### **4. Repeat**\n",
    "- Continue selecting actions and updating estimates.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of UCB**\n",
    "\n",
    "1. **Efficient Exploration**:\n",
    "   - Explicitly incorporates a measure of uncertainty into the action selection.\n",
    "\n",
    "2. **Balanced Strategy**:\n",
    "   - Balances exploration and exploitation naturally through the confidence term.\n",
    "\n",
    "3. **Theoretical Guarantees**:\n",
    "   - Provides bounds on the regret in bandit problems, meaning it converges to the optimal action in the long run.\n",
    "\n",
    "---\n",
    "\n",
    "## **UCB in Gridworld Example**\n",
    "\n",
    "### **Setup**\n",
    "1. **Grid**: A $3 \\times 3$ gridworld.\n",
    "2. **Actions**: Up, Down, Left, Right.\n",
    "3. **Rewards**:\n",
    "   - $-1$ for each step.\n",
    "   - $+10$ for reaching the goal.\n",
    "4. **Exploration**:\n",
    "   - Use UCB to decide which action to take in each state.\n",
    "\n",
    "---\n",
    "\n",
    "## **Explanation of Python Implementation**\n",
    "\n",
    "### **1. Initialization**\n",
    "\n",
    "- **Action-Value Estimates ($Q$)**:\n",
    "  - Initialize $Q(s, a)$ for all states $s$ and actions $a$ to 0.\n",
    "\n",
    "- **Action Counts ($N$)**:\n",
    "  - Initialize $N(s, a)$ to 1 for numerical stability.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. UCB Action Selection**\n",
    "\n",
    "The action with the highest UCB value is selected:\n",
    "$$\n",
    "Q(s, a) + c \\sqrt{\\frac{\\ln t}{N(s, a)}}\n",
    "$$\n",
    "- $Q(s, a)$: Current estimate of the action value.\n",
    "- $c$: Exploration parameter that controls the degree of exploration.\n",
    "- $t$: Current time step.\n",
    "- $N(s, a)$: Number of times the action $a$ has been taken in state $s$.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Q-Value Update**\n",
    "\n",
    "After observing a reward $ r $ from taking action $ a $ in state $ s $, update the value $ Q(s, a) $:\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\frac{1}{N(s, a)} \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)\n",
    "$$\n",
    "- This rule incrementally updates $ Q(s, a) $ based on:\n",
    "  - The observed reward $ r $,\n",
    "  - The discounted future value of the next state $ s' $,\n",
    "  - The number of times $ a $ has been taken.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Policy Extraction**\n",
    "\n",
    "After training, derive the optimal policy by selecting the action with the highest value for each state:\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_a Q(s, a)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of UCB in RL**\n",
    "\n",
    "1. **Efficient Exploration**:\n",
    "   - Systematically explores under-visited actions.\n",
    "\n",
    "2. **Dynamic Adaptation**:\n",
    "   - Balances exploration and exploitation dynamically based on the number of visits.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "- **Initialization**:\n",
    "  - Initialize $Q(s, a)$ and $N(s, a)$.\n",
    "- **Action Selection**:\n",
    "  - Use the UCB formula to select actions based on current estimates and uncertainty.\n",
    "- **Value Updates**:\n",
    "  - Update $Q(s, a)$ incrementally using observed rewards.\n",
    "- **Policy Extraction**:\n",
    "  - Derive the optimal policy based on learned $Q(s, a)$.\n",
    "\n",
    "Would you like to extend this example to more complex environments or discuss UCB in other RL settings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffa3157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy:\n",
      "[['right' 'right' 'down']\n",
      " ['right' 'down' 'down']\n",
      " ['right' 'right' 'up']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters for the gridworld\n",
    "grid_size = (3, 3)\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "gamma = 0.9  # Discount factor\n",
    "c = 1.0  # UCB exploration parameter\n",
    "n_episodes = 500  # Number of episodes\n",
    "\n",
    "# Initialize action-value estimates (Q) and action counts (N) for each state\n",
    "Q = np.zeros((grid_size[0], grid_size[1], len(actions)))  # Action-value estimates\n",
    "N = np.ones((grid_size[0], grid_size[1], len(actions)))  # Action counts (initialize to 1 for stability)\n",
    "\n",
    "# Action mapping\n",
    "action_map = {\n",
    "    \"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)\n",
    "}\n",
    "\n",
    "# Step function for the gridworld\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "    di, dj = action_map[action]\n",
    "    ni, nj = i + di, j + dj\n",
    "    if 0 <= ni < grid_size[0] and 0 <= nj < grid_size[1]:  # Valid move\n",
    "        next_state = (ni, nj)\n",
    "    else:  # Hit a boundary\n",
    "        next_state = state\n",
    "    reward = -1 if next_state != (2, 2) else 10  # -1 per step, +10 for goal\n",
    "    done = next_state == (2, 2)  # Episode ends at the goal\n",
    "    return next_state, reward, done\n",
    "\n",
    "# UCB action selection\n",
    "def ucb_action(state, Q, N, t):\n",
    "    i, j = state\n",
    "    ucb_values = Q[i, j] + c * np.sqrt(np.log(t + 1) / N[i, j])\n",
    "    return np.argmax(ucb_values)\n",
    "\n",
    "# Training using UCB\n",
    "for episode in range(1, n_episodes + 1):\n",
    "    state = (0, 0)  # Start state\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Choose action using UCB\n",
    "        action_index = ucb_action(state, Q, N, episode)\n",
    "        action = actions[action_index]\n",
    "        \n",
    "        # Take action and observe reward\n",
    "        next_state, reward, done = step(state, action)\n",
    "        \n",
    "        # Update action count\n",
    "        i, j = state\n",
    "        N[i, j, action_index] += 1\n",
    "        \n",
    "        # Update Q-value using incremental mean\n",
    "        Q[i, j, action_index] += (reward + gamma * np.max(Q[next_state]) - Q[i, j, action_index]) / N[i, j, action_index]\n",
    "        \n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "\n",
    "# Optimal policy extraction\n",
    "policy = np.zeros((grid_size[0], grid_size[1]), dtype=object)\n",
    "for i in range(grid_size[0]):\n",
    "    for j in range(grid_size[1]):\n",
    "        best_action = np.argmax(Q[i, j])\n",
    "        policy[i, j] = actions[best_action]\n",
    "\n",
    "print(\"Learned Policy:\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda03bf",
   "metadata": {},
   "source": [
    "# Iterative Example: How UCB Works for a Gridworld\n",
    "\n",
    "---\n",
    "\n",
    "## **Gridworld Setup**\n",
    "1. **Grid**: A $3 \\times 3$ grid.\n",
    "2. **Start State**: $(0, 0)$.\n",
    "3. **Goal State**: $(2, 2)$ with a reward of $+10$.\n",
    "4. **Actions**: Up, Down, Left, Right.\n",
    "5. **Rewards**:\n",
    "   - $-1$ for each step.\n",
    "   - $+10$ for reaching the goal.\n",
    "\n",
    "---\n",
    "\n",
    "## **UCB Formula**\n",
    "For each action $a$ in a state $s$:\n",
    "$$\n",
    "Q(s, a) + c \\sqrt{\\frac{\\ln t}{N(s, a)}}\n",
    "$$\n",
    "Where:\n",
    "- $Q(s, a)$: Current estimate of the value of action $a$.\n",
    "- $c$: Exploration parameter (e.g., $c = 1.0$).\n",
    "- $t$: Current time step.\n",
    "- $N(s, a)$: Number of times action $a$ has been selected in state $s$.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step-by-Step Iteration**\n",
    "\n",
    "### **Initialization**\n",
    "- **Action-Value Estimates**:\n",
    "  - $Q(s, a) = 0$ for all states and actions.\n",
    "- **Action Counts**:\n",
    "  - $N(s, a) = 1$ for all actions (for numerical stability).\n",
    "\n",
    "### **Iteration 1**: Time Step $t = 1$\n",
    "- **State**: $(0, 0)$.\n",
    "- **Possible Actions**: Up, Down, Left, Right.\n",
    "- **UCB Calculation** for each action:\n",
    "  - All $Q(s, a) = 0$ and $N(s, a) = 1$:\n",
    "    $$\n",
    "    \\text{UCB}(s, a) = 0 + c \\sqrt{\\frac{\\ln 1}{1}} = 0\n",
    "    $$\n",
    "- **Action Selection**:\n",
    "  - Randomly select one action since all actions have equal UCB.\n",
    "  - Assume \"Down\" is chosen.\n",
    "- **Reward and Update**:\n",
    "  - Moving \"Down\" leads to $(1, 0)$ with a reward of $-1$.\n",
    "  - Update $Q(0, 0, \\text{\"Down\"})$:\n",
    "    $$\n",
    "    Q(0, 0, \\text{\"Down\"}) \\leftarrow -1\n",
    "    $$\n",
    "  - Increment $N(0, 0, \\text{\"Down\"}) = 2$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Iteration 2**: Time Step $t = 2$\n",
    "- **State**: $(1, 0)$.\n",
    "- **Possible Actions**: Up, Down, Left, Right.\n",
    "- **UCB Calculation**:\n",
    "  - Assume $Q(s, a) = 0$ for all actions.\n",
    "  - For each action:\n",
    "    $$\n",
    "    \\text{UCB}(s, a) = 0 + c \\sqrt{\\frac{\\ln 2}{1}} \\approx 1.177\n",
    "    $$\n",
    "- **Action Selection**:\n",
    "  - Randomly select one action, e.g., \"Down.\"\n",
    "- **Reward and Update**:\n",
    "  - Moving \"Down\" leads to $(2, 0)$ with a reward of $-1$.\n",
    "  - Update $Q(1, 0, \\text{\"Down\"})$:\n",
    "    $$\n",
    "    Q(1, 0, \\text{\"Down\"}) \\leftarrow -1\n",
    "    $$\n",
    "  - Increment $N(1, 0, \\text{\"Down\"}) = 2$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Iteration 3**: Time Step $t = 3$\n",
    "- **State**: $(2, 0)$.\n",
    "- **Possible Actions**: Up, Down, Left, Right.\n",
    "- **UCB Calculation**:\n",
    "  - Assume $Q(s, a) = 0$ for all actions.\n",
    "  - For each action:\n",
    "    $$\n",
    "    \\text{UCB}(s, a) = 0 + c \\sqrt{\\frac{\\ln 3}{1}} \\approx 1.482\n",
    "    $$\n",
    "- **Action Selection**:\n",
    "  - Randomly select one action, e.g., \"Right.\"\n",
    "- **Reward and Update**:\n",
    "  - Moving \"Right\" leads to $(2, 1)$ with a reward of $-1$.\n",
    "  - Update $Q(2, 0, \\text{\"Right\"})$:\n",
    "    $$\n",
    "    Q(2, 0, \\text{\"Right\"}) \\leftarrow -1\n",
    "    $$\n",
    "  - Increment $N(2, 0, \\text{\"Right\"}) = 2$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Iteration 4**: Time Step $t = 4$\n",
    "- **State**: $(2, 1)$.\n",
    "- **Possible Actions**: Up, Down, Left, Right.\n",
    "- **UCB Calculation**:\n",
    "  - Assume $Q(s, a) = 0$ for all actions.\n",
    "  - For each action:\n",
    "    $$\n",
    "    \\text{UCB}(s, a) = 0 + c \\sqrt{\\frac{\\ln 4}{1}} \\approx 1.665\n",
    "    $$\n",
    "- **Action Selection**:\n",
    "  - Randomly select one action, e.g., \"Right.\"\n",
    "- **Reward and Update**:\n",
    "  - Moving \"Right\" leads to $(2, 2)$ (goal) with a reward of $+10$.\n",
    "  - Update $Q(2, 1, \\text{\"Right\"})$:\n",
    "    $$\n",
    "    Q(2, 1, \\text{\"Right\"}) \\leftarrow +10\n",
    "    $$\n",
    "  - Increment $N(2, 1, \\text{\"Right\"}) = 2$.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "- **Action-Value Updates**:\n",
    "  - As actions are selected and rewards are observed, $Q(s, a)$ and $N(s, a)$ are updated iteratively.\n",
    "  - Actions leading to the goal state accumulate higher $Q(s, a)$, while others remain low.\n",
    "\n",
    "- **Policy Extraction**:\n",
    "  - After sufficient iterations, derive the policy:\n",
    "    $$\n",
    "    \\pi(s) = \\arg\\max_a Q(s, a)\n",
    "    $$\n",
    "  - The policy converges to the optimal path to the goal.\n",
    "\n",
    "---\n",
    "\n",
    "This example shows how UCB balances exploration and exploitation dynamically based on the confidence bound and observed rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e436b6",
   "metadata": {},
   "source": [
    "# How UCB Changes in the Next Episode\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Updated Values from Previous Episode**\n",
    "\n",
    "### **Action Counts ($N(s, a)$)**\n",
    "- After the first episode:\n",
    "  - Action counts $N(s, a)$ for visited state-action pairs will have increased.\n",
    "  - This means actions taken frequently in the previous episode will now have a lower **confidence term** in the UCB formula:\n",
    "    $$\n",
    "    c \\sqrt{\\frac{\\ln t}{N(s, a)}}\n",
    "    $$\n",
    "  - Actions that were not selected in the previous episode retain their initial values of $N(s, a) = 1$, keeping their confidence term large, encouraging exploration.\n",
    "\n",
    "---\n",
    "\n",
    "### **Action-Value Estimates ($Q(s, a)$)**\n",
    "- After the first episode:\n",
    "  - $Q(s, a)$ for actions taken during the previous episode will now reflect the observed rewards and updated values for the future discounted returns:\n",
    "    $$\n",
    "    Q(s, a) \\leftarrow Q(s, a) + \\frac{1}{N(s, a)} \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)\n",
    "    $$\n",
    "  - Actions that led to better rewards or future states with high values will have larger $Q(s, a)$.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. UCB Action Selection in the Next Episode**\n",
    "\n",
    "### **Exploration vs. Exploitation**\n",
    "- Actions with higher $Q(s, a)$ (better rewards) will now dominate the **exploitation term** in the UCB formula:\n",
    "  $$\n",
    "  Q(s, a) + c \\sqrt{\\frac{\\ln t}{N(s, a)}}\n",
    "  $$\n",
    "- However, actions with smaller $N(s, a)$ (under-explored) will still have large **confidence terms**, encouraging exploration.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Changes**\n",
    "\n",
    "#### **State $(0, 0)$**\n",
    "From the previous episode:\n",
    "- Action \"Down\" might have been selected, updating:\n",
    "  - $N(0, 0, \\text{\"Down\"}) \\to 2$\n",
    "  - $Q(0, 0, \\text{\"Down\"}) \\to -1$ (reflecting the reward observed).\n",
    "- Other actions (\"Up\", \"Left\", \"Right\"):\n",
    "  - Retain $N(0, 0, a) = 1$ and $Q(0, 0, a) = 0$.\n",
    "\n",
    "For the next episode:\n",
    "- **UCB Values**:\n",
    "  $$\n",
    "  \\text{UCB}(0, 0, \\text{\"Down\"}) = -1 + c \\sqrt{\\frac{\\ln 2}{2}} \\approx -1 + 0.588\n",
    "  $$\n",
    "  $$\n",
    "  \\text{UCB}(0, 0, \\text{\"Up\"}) = 0 + c \\sqrt{\\frac{\\ln 2}{1}} \\approx 1.177\n",
    "  $$\n",
    "- **Action Selection**:\n",
    "  - \"Up\" or another untried action will likely be selected due to their higher UCB values.\n",
    "\n",
    "---\n",
    "\n",
    "#### **State $(1, 0)$**\n",
    "From the previous episode:\n",
    "- Action \"Down\" might have been selected, updating:\n",
    "  - $N(1, 0, \\text{\"Down\"}) \\to 2$\n",
    "  - $Q(1, 0, \\text{\"Down\"}) \\to -1$.\n",
    "- Other actions (\"Up\", \"Left\", \"Right\"):\n",
    "  - Retain $N(1, 0, a) = 1$ and $Q(1, 0, a) = 0$.\n",
    "\n",
    "For the next episode:\n",
    "- **UCB Values**:\n",
    "  $$\n",
    "  \\text{UCB}(1, 0, \\text{\"Down\"}) = -1 + c \\sqrt{\\frac{\\ln 2}{2}} \\approx -1 + 0.588\n",
    "  $$\n",
    "  $$\n",
    "  \\text{UCB}(1, 0, \\text{\"Up\"}) = 0 + c \\sqrt{\\frac{\\ln 2}{1}} \\approx 1.177\n",
    "  $$\n",
    "- **Action Selection**:\n",
    "  - Similar to the first state, an under-explored action (\"Up\", \"Left\", or \"Right\") will likely be selected.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Policy Refinement in Future Episodes**\n",
    "\n",
    "- **Repeated Visits**:\n",
    "  - Over multiple episodes, as $N(s, a)$ increases for all actions, the confidence term diminishes for all actions.\n",
    "  - The agent increasingly relies on $Q(s, a)$ to guide action selection (exploitation).\n",
    "\n",
    "- **Optimal Policy**:\n",
    "  - Actions that consistently lead to the goal state with higher rewards will have the highest $Q(s, a)$.\n",
    "  - The learned policy converges to:\n",
    "    $$\n",
    "    \\pi(s) = \\arg\\max_a Q(s, a)\n",
    "    $$\n",
    "  - This policy directs the agent to the goal state efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary of Changes in the Next Episode**\n",
    "\n",
    "1. **Action Selection**:\n",
    "   - Previously untried actions will be prioritized due to their large confidence terms.\n",
    "   - Actions taken in the first episode will have reduced confidence terms but may still dominate due to their updated $Q(s, a)$.\n",
    "\n",
    "2. **Exploration vs. Exploitation**:\n",
    "   - Under-explored actions will continue to be chosen, ensuring thorough exploration of the state-action space.\n",
    "\n",
    "3. **Policy Refinement**:\n",
    "   - Over time, $Q(s, a)$ values dominate, driving the agent to converge on the optimal policy.\n",
    "\n",
    "This iterative process ensures that the agent efficiently balances exploration and exploitation while learning the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc66954",
   "metadata": {},
   "source": [
    "# Explicit Illustration of Action-Value Estimate Updates Between Episodes\n",
    "\n",
    "---\n",
    "\n",
    "## **Equation**:\n",
    "The update rule for $Q(s, a)$ is:\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\frac{1}{N(s, a)} \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Setup**\n",
    "1. **Grid**: A $3 \\times 3$ gridworld.\n",
    "2. **Start State**: $(0, 0)$.\n",
    "3. **Goal State**: $(2, 2)$ with a reward of $+10$.\n",
    "4. **Actions**: Up, Down, Left, Right.\n",
    "5. **Rewards**:\n",
    "   - $-1$ for each step.\n",
    "   - $+10$ for reaching the goal.\n",
    "6. **Discount Factor**: $\\gamma = 0.9$.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example: State $(0, 0)$, Action \"Down\"**\n",
    "\n",
    "### **Initial Values (Before First Episode)**\n",
    "- $Q(0, 0, \\text{\"Down\"}) = 0$\n",
    "- $N(0, 0, \\text{\"Down\"}) = 1$ (initialized to avoid division by zero)\n",
    "\n",
    "---\n",
    "\n",
    "### **First Episode**\n",
    "- Action \"Down\" is selected.\n",
    "- **Reward**: $r = -1$\n",
    "- **Next State**: $(1, 0)$\n",
    "- **Update**:\n",
    "  $$\n",
    "  Q(0, 0, \\text{\"Down\"}) \\leftarrow Q(0, 0, \\text{\"Down\"}) + \\frac{1}{N(0, 0, \\text{\"Down\"})} \\left( r + \\gamma \\max_{a'} Q(1, 0, a') - Q(0, 0, \\text{\"Down\"}) \\right)\n",
    "  $$\n",
    "- Substitute values:\n",
    "  $$\n",
    "  Q(0, 0, \\text{\"Down\"}) \\leftarrow 0 + \\frac{1}{1} \\left( -1 + 0.9 \\cdot 0 - 0 \\right)\n",
    "  $$\n",
    "- Simplify:\n",
    "  $$\n",
    "  Q(0, 0, \\text{\"Down\"}) = -1\n",
    "  $$\n",
    "- Increment:\n",
    "  $$\n",
    "  N(0, 0, \\text{\"Down\"}) = 2\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### **Second Episode**\n",
    "- Action \"Down\" is selected again.\n",
    "- **Reward**: $r = -1$\n",
    "- **Next State**: $(1, 0)$\n",
    "- **Update**:\n",
    "  $$\n",
    "  Q(0, 0, \\text{\"Down\"}) \\leftarrow Q(0, 0, \\text{\"Down\"}) + \\frac{1}{N(0, 0, \\text{\"Down\"})} \\left( r + \\gamma \\max_{a'} Q(1, 0, a') - Q(0, 0, \\text{\"Down\"}) \\right)\n",
    "  $$\n",
    "- Substitute values:\n",
    "  - $Q(0, 0, \\text{\"Down\"}) = -1$\n",
    "  - $N(0, 0, \\text{\"Down\"}) = 2$\n",
    "  - $r = -1$\n",
    "  - $\\gamma = 0.9$\n",
    "  - $\\max_{a'} Q(1, 0, a') = 0$ (no updates for state $(1, 0)$ yet)\n",
    "  $$\n",
    "  Q(0, 0, \\text{\"Down\"}) \\leftarrow -1 + \\frac{1}{2} \\left( -1 + 0.9 \\cdot 0 - (-1) \\right)\n",
    "  $$\n",
    "- Simplify:\n",
    "  $$\n",
    "  Q(0, 0, \\text{\"Down\"}) = -1 + \\frac{1}{2} \\left( 0 \\right)\n",
    "  $$\n",
    "  $$\n",
    "  Q(0, 0, \\text{\"Down\"}) = -1\n",
    "  $$\n",
    "- Increment:\n",
    "  $$\n",
    "  N(0, 0, \\text{\"Down\"}) = 3\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### **Third Episode**\n",
    "- Action \"Down\" is selected again.\n",
    "- **Reward**: $r = -1$\n",
    "- **Next State**: $(1, 0)$\n",
    "- Assume $Q(1, 0, a')$ is updated to $-0.5$ during this episode for some action $a'$.\n",
    "- **Update**:\n",
    "  $$\n",
    "  Q(0, 0, \\text{\"Down\"}) \\leftarrow Q(0, 0, \\text{\"Down\"}) + \\frac{1}{N(0, 0, \\text{\"Down\"})} \\left( r + \\gamma \\max_{a'} Q(1, 0, a') - Q(0, 0, \\text{\"Down\"}) \\right)\n",
    "  $$\n",
    "- Substitute values:\n",
    "  - $Q(0, 0, \\text{\"Down\"}) = -1$\n",
    "  - $N(0, 0, \\text{\"Down\"}) = 3$\n",
    "  - $r = -1$\n",
    "  - $\\gamma = 0.9$\n",
    "  - $\\max_{a'} Q(1, 0, a') = -0.5$\n",
    "  $$\n",
    "  Q(0, 0, \\text{\"Down\"}) \\leftarrow -1 + \\frac{1}{3} \\left( -1 + 0.9 \\cdot (-0.5) - (-1) \\right)\n",
    "  $$\n",
    "- Simplify:\n",
    "  $$\n",
    "  Q(0, 0, \\text{\"Down\"}) \\leftarrow -1 + \\frac{1}{3} \\left( -1 - 0.45 + 1 \\right)\n",
    "  $$\n",
    "  $$\n",
    "  Q(0, 0, \\text{\"Down\"}) \\leftarrow -1 + \\frac{1}{3} \\left( -0.45 \\right)\n",
    "  $$\n",
    "  $$\n",
    "  Q(0, 0, \\text{\"Down\"}) \\leftarrow -1.15\n",
    "  $$\n",
    "- Increment:\n",
    "  $$\n",
    "  N(0, 0, \\text{\"Down\"}) = 4\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary of Changes**\n",
    "\n",
    "After three episodes, the values evolve as follows:\n",
    "\n",
    "|$$Episode$$|$$N(0, 0, \\text{\"Down\"})$$|$$Q(0, 0, \\text{\"Down\"})$$|\n",
    "|---------|--------------------------|--------------------------|\n",
    "| 1       | 2                        | $-1$                    |\n",
    "| 2       | 3                        | $-1$                    |\n",
    "| 3       | 4                        | $-1.15$                 |\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Observations**\n",
    "1. **Confidence Term Shrinks**:\n",
    "   - As $N(s, a)$ increases, the confidence term $\\sqrt{\\ln t / N(s, a)}$ becomes smaller, reducing exploration for that action.\n",
    "\n",
    "2. **Exploitation Becomes Dominant**:\n",
    "   - Updates to $Q(s, a)$ increasingly reflect the observed rewards and future state values, improving the action-value estimates for the optimal policy.\n",
    "\n",
    "3. **Convergence**:\n",
    "   - Over time, $Q(s, a)$ stabilizes, and the learned policy converges to the optimal one.\n",
    "\n",
    "This process continues for all state-action pairs until the agent learns the optimal policy for the gridworld."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064134dc",
   "metadata": {},
   "source": [
    "# Thompson Sampling: An Introduction\n",
    "\n",
    "---\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "**Thompson Sampling** is a Bayesian approach for solving the exploration-exploitation dilemma in **multi-armed bandits** and **reinforcement learning**. Unlike algorithms like **UCB**, which use deterministic confidence bounds, Thompson Sampling uses **probabilistic sampling** to select actions based on their posterior distributions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Idea of Thompson Sampling**\n",
    "\n",
    "The core idea of Thompson Sampling is to **sample an action according to the probability that it is the optimal action**. This is achieved by maintaining a **posterior distribution** over the expected reward of each action and sampling from these distributions to guide exploration.\n",
    "\n",
    "---\n",
    "\n",
    "## **How Thompson Sampling Works**\n",
    "\n",
    "1. **Prior Distribution**:\n",
    "   - Assume a prior distribution over the expected reward for each action.\n",
    "   - Commonly, the **Beta distribution** is used in the case of binary rewards, parameterized as:\n",
    "     $$\n",
    "     \\text{Beta}(\\alpha, \\beta)\n",
    "     $$\n",
    "     - $\\alpha$: Number of successes (reward = 1).\n",
    "     - $\\beta$: Number of failures (reward = 0).\n",
    "\n",
    "2. **Action Sampling**:\n",
    "   - For each action, sample a value from its posterior distribution:\n",
    "     $$\n",
    "     \\theta_a \\sim \\text{Beta}(\\alpha_a, \\beta_a)\n",
    "     $$\n",
    "   - Select the action with the highest sampled value:\n",
    "     $$\n",
    "     A_t = \\arg\\max_a \\theta_a\n",
    "     $$\n",
    "\n",
    "3. **Reward Observation**:\n",
    "   - Observe the reward $r$ for the selected action.\n",
    "   - Update the posterior distribution for that action:\n",
    "     - If $r = 1$ (success), increment $\\alpha_a$.\n",
    "     - If $r = 0$ (failure), increment $\\beta_a$.\n",
    "\n",
    "4. **Repeat**:\n",
    "   - Continue sampling, selecting actions, and updating distributions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages of Thompson Sampling**\n",
    "\n",
    "1. **Efficient Exploration**:\n",
    "   - Naturally balances exploration and exploitation through posterior sampling.\n",
    "\n",
    "2. **Probabilistic Framework**:\n",
    "   - Incorporates uncertainty in the action-value estimates explicitly.\n",
    "\n",
    "3. **Bayesian Updating**:\n",
    "   - Continuously refines the posterior distributions as more data is collected.\n",
    "\n",
    "---\n",
    "\n",
    "## **Thompson Sampling in Gridworld**\n",
    "\n",
    "Let’s extend Thompson Sampling to a **Gridworld** environment where the agent learns the optimal policy probabilistically.\n",
    "\n",
    "---\n",
    "\n",
    "## **Setup**\n",
    "\n",
    "1. **Grid**: A $3 \\times 3$ gridworld.\n",
    "2. **States**: Each cell in the grid represents a state.\n",
    "3. **Actions**: Up, Down, Left, Right.\n",
    "4. **Rewards**:\n",
    "   - $-1$ for each step.\n",
    "   - $+10$ for reaching the goal at $(2, 2)$.\n",
    "5. **Posterior Distribution**:\n",
    "   - Use Beta distributions to model the success probabilities of actions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "557ebfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy:\n",
      "[['up' 'right' 'down']\n",
      " ['up' 'right' 'down']\n",
      " ['right' 'right' 'up']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Gridworld parameters\n",
    "grid_size = (3, 3)\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "n_actions = len(actions)\n",
    "n_episodes = 500\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "# Initialize parameters for Thompson Sampling\n",
    "alpha = np.ones((grid_size[0], grid_size[1], n_actions))  # Success counts\n",
    "beta = np.ones((grid_size[0], grid_size[1], n_actions))   # Failure counts\n",
    "\n",
    "# Action mapping\n",
    "action_map = {\n",
    "    \"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)\n",
    "}\n",
    "\n",
    "# Step function\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "    di, dj = action_map[action]\n",
    "    ni, nj = i + di, j + dj\n",
    "    if 0 <= ni < grid_size[0] and 0 <= nj < grid_size[1]:  # Valid move\n",
    "        next_state = (ni, nj)\n",
    "    else:  # Hit a boundary\n",
    "        next_state = state\n",
    "    reward = -1 if next_state != (2, 2) else 10  # -1 per step, +10 for goal\n",
    "    done = next_state == (2, 2)  # Episode ends at the goal\n",
    "    return next_state, reward, done\n",
    "\n",
    "# Thompson Sampling action selection\n",
    "def thompson_sampling_action(state):\n",
    "    i, j = state\n",
    "    sampled_values = np.random.beta(alpha[i, j], beta[i, j])\n",
    "    return np.argmax(sampled_values)\n",
    "\n",
    "# Training using Thompson Sampling\n",
    "for episode in range(n_episodes):\n",
    "    state = (0, 0)  # Start state\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Select action using Thompson Sampling\n",
    "        action_index = thompson_sampling_action(state)\n",
    "        action = actions[action_index]\n",
    "        \n",
    "        # Take action and observe reward\n",
    "        next_state, reward, done = step(state, action)\n",
    "        \n",
    "        # Update Beta distribution parameters\n",
    "        i, j = state\n",
    "        if reward > 0:  # Success\n",
    "            alpha[i, j, action_index] += 1\n",
    "        else:  # Failure\n",
    "            beta[i, j, action_index] += 1\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "# Extract learned policy\n",
    "policy = np.zeros((grid_size[0], grid_size[1]), dtype=object)\n",
    "for i in range(grid_size[0]):\n",
    "    for j in range(grid_size[1]):\n",
    "        best_action = np.argmax(alpha[i, j] / (alpha[i, j] + beta[i, j]))\n",
    "        policy[i, j] = actions[best_action]\n",
    "\n",
    "print(\"Learned Policy:\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d92546f",
   "metadata": {},
   "source": [
    "## **Explanation of the Code**\n",
    "\n",
    "1. **Initialization**:\n",
    "   - $\\alpha$ and $\\beta$ are initialized to 1 for all state-action pairs, representing uniform priors.\n",
    "\n",
    "2. **Action Selection**:\n",
    "   - For the current state, sample from the Beta distribution for each action.\n",
    "   - Select the action with the highest sampled value.\n",
    "\n",
    "3. **Reward Observation**:\n",
    "   - Observe the reward for the selected action.\n",
    "   - Update the posterior distribution:\n",
    "     - Increment $\\alpha$ for successes.\n",
    "     - Increment $\\beta$ for failures.\n",
    "\n",
    "4. **Policy Extraction**:\n",
    "   - After training, derive the optimal policy by selecting the action with the highest success probability for each state:\n",
    "     $$\n",
    "     \\pi(s) = \\arg\\max_a \\frac{\\alpha(s, a)}{\\alpha(s, a) + \\beta(s, a)}\n",
    "     $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a944b705",
   "metadata": {},
   "source": [
    "# L5: Model-Free Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2697cd",
   "metadata": {},
   "source": [
    "## Model-Free Prediction: Monte Carlo Method\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Monte Carlo (MC) methods are a class of **model-free reinforcement learning algorithms** used for **prediction**. These methods estimate the **value function** of a policy based on sampled episodes from the environment, without requiring a model of the environment’s dynamics.\n",
    "\n",
    "Monte Carlo methods rely on averaging returns over multiple episodes to estimate state or state-action values.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Characteristics of Monte Carlo Methods\n",
    "\n",
    "1. **Model-Free**: \n",
    "   - No need for knowledge of the environment's transition probabilities or rewards.\n",
    "   - Interaction with the environment is direct.\n",
    "\n",
    "2. **Policy Evaluation**:\n",
    "   - Estimates the value function $V^\\pi(s)$ or $Q^\\pi(s, a)$ for a given policy $\\pi$.\n",
    "\n",
    "3. **Episode-Based**:\n",
    "   - Updates are performed only at the end of an episode.\n",
    "   - Assumes the environment is episodic.\n",
    "\n",
    "4. **Return-Based Updates**:\n",
    "   - Value estimates are computed using **returns** (cumulative rewards) observed in sampled episodes.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "To estimate the value function for a given policy $\\pi$:\n",
    "\n",
    "- **State-Value Function**:\n",
    "  $$\n",
    "  V^\\pi(s) = \\mathbb{E}_\\pi [G_t \\mid S_t = s]\n",
    "  $$\n",
    "\n",
    "- **Action-Value Function**:\n",
    "  $$\n",
    "  Q^\\pi(s, a) = \\mathbb{E}_\\pi [G_t \\mid S_t = s, A_t = a]\n",
    "  $$\n",
    "\n",
    "Where $G_t$ is the **return**:\n",
    "$$\n",
    "G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Monte Carlo Prediction Algorithm\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Initialize value function estimates $V(s)$ or $Q(s, a)$.\n",
    "   - Track state-visit counts for averaging.\n",
    "\n",
    "2. **Episode Sampling**:\n",
    "   - Generate episodes following the policy $\\pi$.\n",
    "   - Record the sequence of states, actions, and rewards.\n",
    "\n",
    "3. **Return Calculation**:\n",
    "   - For each state or state-action pair in the episode, compute the return $G_t$ starting from that point.\n",
    "\n",
    "4. **Value Function Update**:\n",
    "   - Update the value estimates using observed returns:\n",
    "     - Incremental Update:\n",
    "       $$\n",
    "       V(s) \\leftarrow V(s) + \\alpha \\cdot (G_t - V(s))\n",
    "       $$\n",
    "     - Averaging:\n",
    "       $$\n",
    "       V(s) = \\frac{\\text{Sum of Returns}}{\\text{Number of Visits to } s}\n",
    "       $$\n",
    "\n",
    "5. **Repeat**:\n",
    "   - Iterate over multiple episodes until the value function converges.\n",
    "\n",
    "---\n",
    "\n",
    "## Monte Carlo Variants\n",
    "\n",
    "1. **First-Visit Monte Carlo**:\n",
    "   - Updates the value function based only on the **first occurrence** of each state in an episode.\n",
    "\n",
    "2. **Every-Visit Monte Carlo**:\n",
    "   - Updates the value function every time a state is encountered in an episode.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points in the Code\n",
    "\n",
    "1. **Initialization**:\n",
    "   - The value function $V(s)$ is initialized to zero.\n",
    "   - A random policy is used.\n",
    "\n",
    "2. **Episode Generation**:\n",
    "   - For each episode, the agent follows the policy until reaching the goal.\n",
    "\n",
    "3. **Return Calculation**:\n",
    "   - $G_t$ is computed for every state based on the observed rewards.\n",
    "\n",
    "4. **First-Visit Update**:\n",
    "   - Value estimates $V(s)$ are updated using the mean of observed returns for the **first visit** to each state.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3faedd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated State-Value Function:\n",
      "[-6.29157119 -5.08043101 -4.04962497 -3.28498465]\n",
      "[-5.23198789 -3.38965434 -1.09569803  0.51657312]\n",
      "[-4.08331037 -1.17990369  2.38305034  6.52693773]\n",
      "[-3.44679101  0.2846658   5.47514266  0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Gridworld parameters\n",
    "grid_size = (4, 4)\n",
    "gamma = 0.9  # Discount factor\n",
    "n_episodes = 500  # Number of episodes\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros(grid_size)\n",
    "\n",
    "# Define policy (uniform random policy)\n",
    "def policy(state):\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    return np.random.choice(actions)\n",
    "\n",
    "# Action mapping\n",
    "action_map = {\n",
    "    \"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)\n",
    "}\n",
    "\n",
    "# Step function\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "    di, dj = action_map[action]\n",
    "    ni, nj = i + di, j + dj\n",
    "    if 0 <= ni < grid_size[0] and 0 <= nj < grid_size[1]:  # Valid move\n",
    "        next_state = (ni, nj)\n",
    "    else:  # Hit a boundary\n",
    "        next_state = state\n",
    "    reward = -1 if next_state != (3, 3) else 10  # -1 per step, +10 for goal\n",
    "    done = next_state == (3, 3)\n",
    "    return next_state, reward, done\n",
    "\n",
    "# Monte Carlo Prediction (First-Visit)\n",
    "def monte_carlo_prediction(V, n_episodes, gamma):\n",
    "    returns = defaultdict(list)  # Track returns for each state\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state = (0, 0)\n",
    "        episode_data = []\n",
    "        done = False\n",
    "\n",
    "        # Generate an episode\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done = step(state, action)\n",
    "            episode_data.append((state, reward))\n",
    "            state = next_state\n",
    "\n",
    "        # Compute returns\n",
    "        G = 0\n",
    "        visited = set()  # Track first visits\n",
    "        for state, reward in reversed(episode_data):\n",
    "            G = reward + gamma * G\n",
    "            if state not in visited:\n",
    "                returns[state].append(G)\n",
    "                visited.add(state)\n",
    "                # Update value function as average of returns\n",
    "                V[state] = np.mean(returns[state])\n",
    "\n",
    "    return V\n",
    "\n",
    "# Run Monte Carlo Prediction\n",
    "V = monte_carlo_prediction(V, n_episodes, gamma)\n",
    "\n",
    "# Print the estimated value function\n",
    "print(\"Estimated State-Value Function:\")\n",
    "for i in range(grid_size[0]):\n",
    "    print(V[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326a6e3",
   "metadata": {},
   "source": [
    "# Temporal Difference (TD) Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Temporal Difference (TD) learning is a fundamental model-free reinforcement learning method that combines the ideas of **Monte Carlo (MC)** methods and **Dynamic Programming (DP)**. It is used for **prediction** and **control**, offering an efficient way to estimate value functions directly from interactions with the environment.\n",
    "\n",
    "Unlike MC methods, TD learning updates value estimates incrementally after each step, without requiring the completion of an entire episode.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features of TD Learning\n",
    "\n",
    "1. **Model-Free**:\n",
    "   - Does not require knowledge of the environment’s dynamics (transition probabilities or rewards).\n",
    "\n",
    "2. **Online Updates**:\n",
    "   - Updates value estimates incrementally after every step, rather than waiting until the end of an episode (as in MC methods).\n",
    "\n",
    "3. **Bootstraping**:\n",
    "   - Uses the current value function estimate to update itself:\n",
    "     $$\n",
    "     V(S_t) \\leftarrow V(S_t) + \\alpha \\left( R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right)\n",
    "     $$\n",
    "     - $R_{t+1}$: Observed reward at the next step.\n",
    "     - $V(S_{t+1})$: Current estimate of the value of the next state.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "To estimate the **value function** for a given policy $\\pi$:\n",
    "\n",
    "- **State-Value Function**:\n",
    "  $$\n",
    "  V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R_t \\mid S_0 = s \\right]\n",
    "  $$\n",
    "\n",
    "- **Action-Value Function**:\n",
    "  $$\n",
    "  Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R_t \\mid S_0 = s, A_0 = a \\right]\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of TD Learning\n",
    "\n",
    "1. **Efficiency**:\n",
    "   - Updates after every time step, providing faster learning compared to MC methods.\n",
    "\n",
    "2. **Flexibility**:\n",
    "   - Works in both episodic and non-episodic tasks.\n",
    "\n",
    "3. **Combines Sampling and Bootstrapping**:\n",
    "   - Combines Monte Carlo’s sampling of rewards with Dynamic Programming’s bootstrapping to improve convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## TD Prediction Algorithm\n",
    "\n",
    "1. **Initialize**:\n",
    "   - Arbitrary values for $V(s)$ for all states $s$ (e.g., $V(s) = 0$).\n",
    "   - Learning rate $\\alpha$ and discount factor $\\gamma$.\n",
    "\n",
    "2. **Generate an Episode**:\n",
    "   - Start from an initial state and follow the policy $\\pi$.\n",
    "\n",
    "3. **For Each Step**:\n",
    "   - Observe $S_t$, $R_{t+1}$, and $S_{t+1}$.\n",
    "   - Update the value function using the TD update rule:\n",
    "     $$\n",
    "     V(S_t) \\leftarrow V(S_t) + \\alpha \\left( R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right)\n",
    "     $$\n",
    "\n",
    "4. **Repeat**:\n",
    "   - Iterate until $V(s)$ converges for all states $s$.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison with Monte Carlo\n",
    "\n",
    "| Feature                 | Monte Carlo (MC)          | Temporal Difference (TD)   |\n",
    "|-------------------------|---------------------------|-----------------------------|\n",
    "| **Update Timing**       | At the end of an episode  | After each step             |\n",
    "| **Environment Type**    | Episodic only             | Episodic or continuous      |\n",
    "| **Convergence**         | Slow                      | Faster                      |\n",
    "| **Uses Bootstrapping**  | No                        | Yes                         |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ede8a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated State-Value Function:\n",
      "[-8.56420697 -8.18457874 -7.93384257 -7.72542817]\n",
      "[-8.31644831 -7.78480264 -6.82945949 -6.30235826]\n",
      "[-7.60994851 -7.281787   -4.2953795  -1.13278305]\n",
      "[-6.59626537 -4.98515258  0.36602331  0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Gridworld parameters\n",
    "grid_size = (4, 4)\n",
    "gamma = 0.9  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "n_episodes = 500  # Number of episodes\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros(grid_size)\n",
    "\n",
    "# Define policy (uniform random policy)\n",
    "def policy(state):\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    return np.random.choice(actions)\n",
    "\n",
    "# Action mapping\n",
    "action_map = {\n",
    "    \"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)\n",
    "}\n",
    "\n",
    "# Step function\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "    di, dj = action_map[action]\n",
    "    ni, nj = i + di, j + dj\n",
    "    if 0 <= ni < grid_size[0] and 0 <= nj < grid_size[1]:  # Valid move\n",
    "        next_state = (ni, nj)\n",
    "    else:  # Hit a boundary\n",
    "        next_state = state\n",
    "    reward = -1 if next_state != (3, 3) else 10  # -1 per step, +10 for goal\n",
    "    done = next_state == (3, 3)\n",
    "    return next_state, reward, done\n",
    "\n",
    "# TD(0) Prediction\n",
    "def td_prediction(V, n_episodes, alpha, gamma):\n",
    "    for episode in range(n_episodes):\n",
    "        state = (0, 0)  # Start state\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Select an action\n",
    "            action = policy(state)\n",
    "            # Take a step\n",
    "            next_state, reward, done = step(state, action)\n",
    "            # TD(0) update\n",
    "            i, j = state\n",
    "            ni, nj = next_state\n",
    "            V[i, j] += alpha * (reward + gamma * V[ni, nj] - V[i, j])\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "    return V\n",
    "\n",
    "# Run TD(0) Prediction\n",
    "V = td_prediction(V, n_episodes, alpha, gamma)\n",
    "\n",
    "# Print the estimated value function\n",
    "print(\"Estimated State-Value Function:\")\n",
    "for i in range(grid_size[0]):\n",
    "    print(V[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0dfb77",
   "metadata": {},
   "source": [
    "# TD(λ): Temporal Difference with Eligibility Traces\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "TD(λ) is an extension of the **Temporal Difference (TD)** method that combines **Monte Carlo (MC)** and TD learning. It introduces the concept of **eligibility traces**, which allow credit to be assigned not only to the current state but also to recently visited states. This approach enables faster learning and more accurate updates.\n",
    "\n",
    "TD(λ) bridges the gap between **Monte Carlo** and **TD(0)**, controlled by the parameter \\( \\lambda \\) (ranging from \\( 0 \\) to \\( 1 \\)).\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Eligibility Traces**:\n",
    "   - An **eligibility trace** is a mechanism to keep track of the states (or state-action pairs) that have been visited recently.\n",
    "   - It assigns a degree of \"credit\" to each state, which decays over time as the agent moves forward.\n",
    "\n",
    "2. **λ Parameter**:\n",
    "   - \\( \\lambda \\) determines how much past states influence the update:\n",
    "     - \\( \\lambda = 0 \\): Reduces to standard TD(0), where only the current state is updated.\n",
    "     - \\( \\lambda = 1 \\): Approximates Monte Carlo, where the full return is used for all states in the episode.\n",
    "     - \\( 0 < \\lambda < 1 \\): Balances the influence between immediate and longer-term states.\n",
    "\n",
    "3. **Forward and Backward View**:\n",
    "   - **Forward View**:\n",
    "     - Uses \\( \\lambda \\)-returns to compute updates, aggregating future rewards with decreasing weight.\n",
    "   - **Backward View**:\n",
    "     - Uses eligibility traces to assign credit to states incrementally, based on how recently they were visited.\n",
    "\n",
    "---\n",
    "\n",
    "## TD(λ) Update Rule\n",
    "\n",
    "1. **Eligibility Traces**:\n",
    "   - Initialize \\( e(s) = 0 \\) for all states.\n",
    "   - Update at each step:\n",
    "     $$\n",
    "     e(s) = \\begin{cases} \n",
    "     1 & \\text{if } s = S_t \\text{ (current state)} \\\\\n",
    "     \\gamma \\lambda e(s) & \\text{otherwise}\n",
    "     \\end{cases}\n",
    "     $$\n",
    "\n",
    "2. **Value Update**:\n",
    "   - Update the value function incrementally for all states:\n",
    "     $$\n",
    "     V(s) \\leftarrow V(s) + \\alpha \\delta e(s)\n",
    "     $$\n",
    "   - Temporal Difference Error \\( \\delta \\):\n",
    "     $$\n",
    "     \\delta = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\n",
    "     $$\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of TD(λ)\n",
    "\n",
    "1. **Faster Convergence**:\n",
    "   - Combines the strengths of TD(0) and Monte Carlo, leading to faster convergence.\n",
    "\n",
    "2. **Balance of Short- and Long-Term Credit Assignment**:\n",
    "   - By adjusting \\( \\lambda \\), TD(λ) can prioritize short-term or long-term states.\n",
    "\n",
    "3. **Applicability**:\n",
    "   - Suitable for both episodic and continuous tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## TD(λ) Algorithm\n",
    "\n",
    "1. **Initialize**:\n",
    "   - Value estimates \\( V(s) = 0 \\) for all states \\( s \\).\n",
    "   - Eligibility traces \\( e(s) = 0 \\) for all states \\( s \\).\n",
    "\n",
    "2. **Generate an Episode**:\n",
    "   - Start in an initial state and follow the policy \\( \\pi \\).\n",
    "\n",
    "3. **For Each Step**:\n",
    "   - Observe \\( S_t, R_{t+1}, S_{t+1} \\).\n",
    "   - Compute the TD error:\n",
    "     $$\n",
    "     \\delta = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\n",
    "     $$\n",
    "   - Update eligibility traces:\n",
    "     $$\n",
    "     e(S_t) = 1 \\quad \\text{(replace traces)} \\quad \\text{or} \\quad e(S_t) += 1 \\quad \\text{(accumulating traces)}\n",
    "     $$\n",
    "   - Update the value function for all states:\n",
    "     $$\n",
    "     V(s) \\leftarrow V(s) + \\alpha \\delta e(s)\n",
    "     $$\n",
    "   - Decay eligibility traces:\n",
    "     $$\n",
    "     e(s) \\leftarrow \\gamma \\lambda e(s) \\quad \\forall s\n",
    "     $$\n",
    "\n",
    "4. **Repeat**:\n",
    "   - Continue until the value function converges for all states.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Observations\n",
    "\n",
    "1. **Eligibility Traces**:\n",
    "   - Tracks the influence of recent states on the updates.\n",
    "\n",
    "2. **Efficiency**:\n",
    "   - Balances immediate and long-term updates, leading to faster convergence.\n",
    "\n",
    "3. **Parameter \\( \\lambda \\)**:\n",
    "   - Adjusting \\( \\lambda \\) changes the balance between TD(0) and Monte Carlo updates.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "179424fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated State-Value Function:\n",
      "[-9.03652603 -8.86822037 -6.96667623 -6.68097114]\n",
      "[-8.8454756  -7.99764672 -7.00441451 -4.67165171]\n",
      "[-8.48955683 -7.95501614 -6.02035977 -1.05772531]\n",
      "[-6.21169933 -5.42954039 -4.05958491  0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Gridworld parameters\n",
    "grid_size = (4, 4)\n",
    "gamma = 0.9  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "lambda_ = 0.8  # Lambda for TD(lambda)\n",
    "n_episodes = 500  # Number of episodes\n",
    "\n",
    "# Initialize value function\n",
    "V = np.zeros(grid_size)\n",
    "\n",
    "# Define policy (uniform random policy)\n",
    "def policy(state):\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    return np.random.choice(actions)\n",
    "\n",
    "# Action mapping\n",
    "action_map = {\n",
    "    \"up\": (-1, 0), \"down\": (1, 0), \"left\": (0, -1), \"right\": (0, 1)\n",
    "}\n",
    "\n",
    "# Step function\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "    di, dj = action_map[action]\n",
    "    ni, nj = i + di, j + dj\n",
    "    if 0 <= ni < grid_size[0] and 0 <= nj < grid_size[1]:  # Valid move\n",
    "        next_state = (ni, nj)\n",
    "    else:  # Hit a boundary\n",
    "        next_state = state\n",
    "    reward = -1 if next_state != (3, 3) else 10  # -1 per step, +10 for goal\n",
    "    done = next_state == (3, 3)\n",
    "    return next_state, reward, done\n",
    "\n",
    "# TD(lambda) Prediction\n",
    "def td_lambda_prediction(V, n_episodes, alpha, gamma, lambda_):\n",
    "    for episode in range(n_episodes):\n",
    "        state = (0, 0)  # Start state\n",
    "        eligibility = np.zeros_like(V)  # Initialize eligibility traces\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Select an action\n",
    "            action = policy(state)\n",
    "            # Take a step\n",
    "            next_state, reward, done = step(state, action)\n",
    "            # Compute TD error\n",
    "            i, j = state\n",
    "            ni, nj = next_state\n",
    "            td_error = reward + gamma * V[ni, nj] - V[i, j]\n",
    "            # Update eligibility traces\n",
    "            eligibility[i, j] += 1\n",
    "            # Update value function\n",
    "            V += alpha * td_error * eligibility\n",
    "            # Decay eligibility traces\n",
    "            eligibility *= gamma * lambda_\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "    return V\n",
    "\n",
    "# Run TD(lambda) Prediction\n",
    "V = td_lambda_prediction(V, n_episodes, alpha, gamma, lambda_)\n",
    "\n",
    "# Print the estimated value function\n",
    "print(\"Estimated State-Value Function:\")\n",
    "for i in range(grid_size[0]):\n",
    "    print(V[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84951f",
   "metadata": {},
   "source": [
    "# L6 Model-Free Control: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0aebef",
   "metadata": {},
   "source": [
    "## Monte Carlo Control\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Monte Carlo (MC) Control is a **model-free reinforcement learning method** used for learning an optimal policy in environments where the agent interacts with the environment by sampling episodes. MC Control focuses on **estimating action-value functions** and **improving policies** based on these estimates.\n",
    "\n",
    "Unlike **Monte Carlo Prediction**, which estimates the value function under a fixed policy, Monte Carlo Control **optimizes the policy** iteratively by alternating between **policy evaluation** and **policy improvement**.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Monte Carlo Control seeks to find the **optimal policy** $ \\pi^* $ by estimating the **optimal action-value function** $ Q^*(s, a) $:\n",
    "- **Action-Value Function**:\n",
    "  $$\n",
    "  Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s, A_t = a \\right]\n",
    "  $$\n",
    "- **Optimal Action-Value Function**:\n",
    "  $$\n",
    "  Q^*(s, a) = \\max_\\pi Q^\\pi(s, a)\n",
    "  $$\n",
    "- The optimal policy is derived from $ Q^*(s, a) $:\n",
    "  $$\n",
    "  \\pi^*(s) = \\arg\\max_a Q^*(s, a)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Exploration vs. Exploitation**:\n",
    "   - Monte Carlo methods require **sufficient exploration** to estimate action values accurately.\n",
    "   - This is often ensured using an **$ \\epsilon $-greedy policy**, where the agent selects the action with the highest $ Q(s, a) $ most of the time, but occasionally explores other actions with probability $ \\epsilon $.\n",
    "\n",
    "2. **Policy Iteration**:\n",
    "   - **Policy Evaluation**:\n",
    "     - Estimate $ Q(s, a) $ under the current policy using sampled returns.\n",
    "   - **Policy Improvement**:\n",
    "     - Update the policy to be greedy with respect to $ Q(s, a) $:\n",
    "       $$\n",
    "       \\pi(s) = \\arg\\max_a Q(s, a)\n",
    "       $$\n",
    "\n",
    "3. **First-Visit vs. Every-Visit MC**:\n",
    "   - **First-Visit**:\n",
    "     - Updates the action-value function based on the first occurrence of a state-action pair in an episode.\n",
    "   - **Every-Visit**:\n",
    "     - Updates the action-value function every time a state-action pair is encountered in an episode.\n",
    "\n",
    "---\n",
    "\n",
    "## Monte Carlo Control Algorithm\n",
    "\n",
    "1. **Initialize**:\n",
    "   - Action-value estimates $ Q(s, a) $ arbitrarily (e.g., $ Q(s, a) = 0 $).\n",
    "   - Initialize policy $ \\pi(s) $ (e.g., randomly).\n",
    "\n",
    "2. **Generate Episodes**:\n",
    "   - Use the current policy $ \\pi $ to generate episodes by interacting with the environment.\n",
    "\n",
    "3. **Compute Returns**:\n",
    "   - For each state-action pair $ (s, a) $ in the episode, compute the return $ G_t $:\n",
    "     $$\n",
    "     G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n",
    "     $$\n",
    "\n",
    "4. **Update Action-Value Function**:\n",
    "   - Update $ Q(s, a) $ by averaging the returns:\n",
    "     $$\n",
    "     Q(s, a) = \\frac{\\text{sum of returns for } (s, a)}{\\text{number of visits to } (s, a)}\n",
    "     $$\n",
    "\n",
    "5. **Improve Policy**:\n",
    "   - Update $ \\pi(s) $ to be greedy with respect to $ Q(s, a) $:\n",
    "     $$\n",
    "     \\pi(s) = \\arg\\max_a Q(s, a)\n",
    "     $$\n",
    "\n",
    "6. **Repeat**:\n",
    "   - Continue iterating between policy evaluation and policy improvement.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Observations\n",
    "\n",
    "1. **Exploration vs. Exploitation**:\n",
    "   - The $ \\epsilon $-greedy policy ensures sufficient exploration to avoid suboptimal policies.\n",
    "\n",
    "2. **Incremental Updates**:\n",
    "   - $ Q(s, a) $ is updated incrementally based on observed returns.\n",
    "\n",
    "3. **Policy Improvement**:\n",
    "   - The policy is refined iteratively based on the updated action-value function.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to explore other control methods, such as SARSA or Q-Learning? Or extend Monte Carlo methods to more complex environments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b66727fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy:\n",
      "State (0, 0): right\n",
      "State (0, 1): right\n",
      "State (0, 2): right\n",
      "State (0, 3): down\n",
      "State (1, 0): up\n",
      "State (1, 1): up\n",
      "State (1, 2): right\n",
      "State (1, 3): down\n",
      "State (2, 0): up\n",
      "State (2, 1): up\n",
      "State (2, 2): left\n",
      "State (2, 3): down\n",
      "State (3, 0): down\n",
      "State (3, 1): down\n",
      "State (3, 2): down\n",
      "State (3, 3): N/A\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Gridworld parameters\n",
    "grid_size = (4, 4)\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration probability\n",
    "n_episodes = 500  # Number of episodes\n",
    "\n",
    "# Initialize action-value function\n",
    "Q = defaultdict(lambda: np.zeros(4))\n",
    "\n",
    "# Define ε-greedy policy\n",
    "def epsilon_greedy_policy(state, Q, epsilon):\n",
    "    actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    n_actions = len(actions)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(n_actions)  # Explore\n",
    "    else:\n",
    "        return np.argmax(Q[state])  # Exploit\n",
    "\n",
    "# Action mapping\n",
    "action_map = {\n",
    "    0: (-1, 0),  # up\n",
    "    1: (1, 0),   # down\n",
    "    2: (0, -1),  # left\n",
    "    3: (0, 1)    # right\n",
    "}\n",
    "\n",
    "# Step function\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "    di, dj = action_map[action]\n",
    "    ni, nj = i + di, j + dj\n",
    "    if 0 <= ni < grid_size[0] and 0 <= nj < grid_size[1]:  # Valid move\n",
    "        next_state = (ni, nj)\n",
    "    else:  # Hit a boundary\n",
    "        next_state = state\n",
    "    reward = -1 if next_state != (3, 3) else 10  # -1 per step, +10 for goal\n",
    "    done = next_state == (3, 3)\n",
    "    return next_state, reward, done\n",
    "\n",
    "# Monte Carlo Control\n",
    "def monte_carlo_control(n_episodes, gamma, epsilon):\n",
    "    returns_sum = defaultdict(lambda: np.zeros(4))  # Sum of returns\n",
    "    returns_count = defaultdict(lambda: np.zeros(4))  # Count of visits\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        # Generate an episode\n",
    "        state = (0, 0)\n",
    "        episode_data = []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = epsilon_greedy_policy(state, Q, epsilon)\n",
    "            next_state, reward, done = step(state, action)\n",
    "            episode_data.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        # Compute returns\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for state, action, reward in reversed(episode_data):\n",
    "            G = reward + gamma * G\n",
    "            if (state, action) not in visited:\n",
    "                visited.add((state, action))\n",
    "                returns_sum[state][action] += G\n",
    "                returns_count[state][action] += 1\n",
    "                Q[state][action] = returns_sum[state][action] / returns_count[state][action]\n",
    "\n",
    "    # Derive the policy\n",
    "    policy = {}\n",
    "    for state in Q:\n",
    "        policy[state] = np.argmax(Q[state])\n",
    "    return policy, Q\n",
    "\n",
    "# Run Monte Carlo Control\n",
    "policy, Q = monte_carlo_control(n_episodes, gamma, epsilon)\n",
    "\n",
    "# Print the learned policy\n",
    "print(\"Learned Policy:\")\n",
    "for i in range(grid_size[0]):\n",
    "    for j in range(grid_size[1]):\n",
    "        if (i, j) in policy:\n",
    "            action = [\"up\", \"down\", \"left\", \"right\"][policy[(i, j)]]\n",
    "            print(f\"State ({i}, {j}): {action}\")\n",
    "        else:\n",
    "            print(f\"State ({i}, {j}): N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d1fad",
   "metadata": {},
   "source": [
    "# SARSA: State-Action-Reward-State-Action\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "SARSA (State-Action-Reward-State-Action) is an **on-policy, model-free reinforcement learning algorithm** used for learning a policy in an environment. It estimates the **action-value function** $Q(s, a)$ under the current policy by interacting with the environment. SARSA gets its name from the quintuple of events it uses for learning: $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$.\n",
    "\n",
    "Unlike **Q-learning** (an off-policy method), SARSA updates its action-value estimates using the action actually taken under the current policy, ensuring it learns the value of the current policy.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "SARSA seeks to learn the **optimal policy** $\\pi^*$ by estimating the **optimal action-value function** $Q^*(s, a)$:\n",
    "- **Action-Value Function**:\n",
    "  $$\n",
    "  Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a \\right]\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## SARSA Update Rule\n",
    "\n",
    "SARSA updates the action-value function $Q(s, a)$ based on the current state-action pair and the subsequent state-action pair:\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]\n",
    "$$\n",
    "Where:\n",
    "- $\\alpha$: Learning rate.\n",
    "- $\\gamma$: Discount factor.\n",
    "- $Q(S_t, A_t)$: Current estimate of the action-value function for state $S_t$ and action $A_t$.\n",
    "- $Q(S_{t+1}, A_{t+1})$: Estimate of the action-value function for the next state $S_{t+1}$ and action $A_{t+1}$.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Characteristics\n",
    "\n",
    "1. **On-Policy**:\n",
    "   - SARSA learns the value of the policy being followed, rather than the value of an optimal policy.\n",
    "\n",
    "2. **Exploration vs. Exploitation**:\n",
    "   - Typically employs an **$ \\epsilon $-greedy policy** to balance exploration and exploitation.\n",
    "\n",
    "3. **Updates Using Realized Actions**:\n",
    "   - Updates $Q(s, a)$ using the action $A_{t+1}$ that was actually selected by the current policy.\n",
    "\n",
    "---\n",
    "\n",
    "## SARSA Algorithm\n",
    "\n",
    "1. **Initialize**:\n",
    "   - Arbitrary action-value estimates $Q(s, a)$.\n",
    "   - Policy $\\pi(s)$ (e.g., $ \\epsilon $-greedy based on $Q(s, a)$).\n",
    "\n",
    "2. **For Each Episode**:\n",
    "   - Initialize state $S_0$.\n",
    "   - Choose action $A_0$ using policy $\\pi(s)$.\n",
    "\n",
    "3. **For Each Step in the Episode**:\n",
    "   - Take action $A_t$, observe $R_{t+1}$ and $S_{t+1}$.\n",
    "   - Choose $A_{t+1}$ using policy $\\pi(s)$.\n",
    "   - Update $Q(S_t, A_t)$:\n",
    "     $$\n",
    "     Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]\n",
    "     $$\n",
    "   - Set $S_t = S_{t+1}, A_t = A_{t+1}$.\n",
    "\n",
    "4. **Repeat**:\n",
    "   - Continue until convergence of $Q(s, a)$ and the policy.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison: SARSA vs. Q-Learning\n",
    "\n",
    "| Feature            | SARSA (On-Policy)                                     | Q-Learning (Off-Policy)                              |\n",
    "|--------------------|-------------------------------------------------------|-----------------------------------------------------|\n",
    "| **Policy**         | Learns the value of the current policy being followed | Learns the value of the optimal policy              |\n",
    "| **Update Rule**    | Uses $Q(S_{t+1}, A_{t+1})$ for updates                | Uses $\\max_a Q(S_{t+1}, a)$ for updates             |\n",
    "| **Exploration**    | Balances exploration and exploitation in learning     | Exploitation dominates (due to $\\max$ operator)     |\n",
    "| **Application**    | Safer for stochastic environments                     | Potentially more efficient in deterministic tasks   |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Observations\n",
    "\n",
    "1. **Exploration vs. Exploitation**:\n",
    "   - SARSA ensures sufficient exploration through the $ \\epsilon $-greedy policy.\n",
    "\n",
    "2. **On-Policy Learning**:\n",
    "   - Learns the value of the policy it is following, making it robust in stochastic environments.\n",
    "\n",
    "3. **Incremental Updates**:\n",
    "   - Updates $Q(s, a)$ after each step, leading to faster learning compared to Monte Carlo methods.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbbd7620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy:\n",
      "[3 1 1 1]\n",
      "[3 3 3 1]\n",
      "[3 3 3 1]\n",
      "[1 3 3 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Gridworld parameters\n",
    "grid_size = (4, 4)\n",
    "gamma = 0.9  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1  # Exploration probability\n",
    "n_episodes = 500  # Number of episodes\n",
    "\n",
    "# Initialize action-value function\n",
    "Q = np.zeros((grid_size[0], grid_size[1], 4))  # 4 actions: up, down, left, right\n",
    "\n",
    "# Define ε-greedy policy\n",
    "def epsilon_greedy_policy(state, Q, epsilon):\n",
    "    i, j = state\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(4)  # Explore\n",
    "    else:\n",
    "        return np.argmax(Q[i, j])  # Exploit\n",
    "\n",
    "# Action mapping\n",
    "action_map = {\n",
    "    0: (-1, 0),  # up\n",
    "    1: (1, 0),   # down\n",
    "    2: (0, -1),  # left\n",
    "    3: (0, 1)    # right\n",
    "}\n",
    "\n",
    "# Step function\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "    di, dj = action_map[action]\n",
    "    ni, nj = i + di, j + dj\n",
    "    if 0 <= ni < grid_size[0] and 0 <= nj < grid_size[1]:  # Valid move\n",
    "        next_state = (ni, nj)\n",
    "    else:  # Hit a boundary\n",
    "        next_state = state\n",
    "    reward = -1 if next_state != (3, 3) else 10  # -1 per step, +10 for goal\n",
    "    done = next_state == (3, 3)\n",
    "    return next_state, reward, done\n",
    "\n",
    "# SARSA Algorithm\n",
    "def sarsa(Q, n_episodes, alpha, gamma, epsilon):\n",
    "    for episode in range(n_episodes):\n",
    "        # Initialize state and action\n",
    "        state = (0, 0)\n",
    "        action = epsilon_greedy_policy(state, Q, epsilon)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Take action, observe reward and next state\n",
    "            next_state, reward, done = step(state, action)\n",
    "            # Choose next action\n",
    "            next_action = epsilon_greedy_policy(next_state, Q, epsilon)\n",
    "            # Update Q(S_t, A_t)\n",
    "            i, j = state\n",
    "            ni, nj = next_state\n",
    "            Q[i, j, action] += alpha * (reward + gamma * Q[ni, nj, next_action] - Q[i, j, action])\n",
    "            # Move to the next state and action\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "    return Q\n",
    "\n",
    "# Run SARSA\n",
    "Q = sarsa(Q, n_episodes, alpha, gamma, epsilon)\n",
    "\n",
    "# Derive policy\n",
    "policy = np.argmax(Q, axis=2)\n",
    "print(\"Learned Policy:\")\n",
    "for i in range(grid_size[0]):\n",
    "    print(policy[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a5d39",
   "metadata": {},
   "source": [
    "# Q-Learning: Off-Policy Temporal Difference Control\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Q-learning** is a model-free, off-policy reinforcement learning algorithm used to learn the **optimal policy** by estimating the **optimal action-value function**, $Q^*(s, a)$. It is one of the most widely used RL algorithms due to its simplicity and effectiveness.\n",
    "\n",
    "Unlike SARSA (an on-policy algorithm), Q-learning updates its action-value function using the **maximum possible reward from the next state** rather than the action actually taken. This makes Q-learning an **off-policy method**.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Q-learning seeks to find the **optimal policy** $\\pi^*$ by learning the **optimal action-value function** $Q^*(s, a)$, defined as:\n",
    "$$\n",
    "Q^*(s, a) = \\max_\\pi \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a \\right]\n",
    "$$\n",
    "The optimal policy $\\pi^*$ is derived from $Q^*(s, a)$:\n",
    "$$\n",
    "\\pi^*(s) = \\arg\\max_a Q^*(s, a)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Q-Learning Update Rule\n",
    "\n",
    "Q-learning updates the action-value function $Q(s, a)$ using the **Bellman optimality equation**:\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right]\n",
    "$$\n",
    "Where:\n",
    "- $\\alpha$: Learning rate (controls the step size of updates).\n",
    "- $\\gamma$: Discount factor (controls the importance of future rewards).\n",
    "- $R_{t+1}$: Immediate reward after taking action $A_t$ in state $S_t$.\n",
    "- $\\max_a Q(S_{t+1}, a)$: Estimate of the optimal future value from the next state $S_{t+1}$.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Characteristics\n",
    "\n",
    "1. **Off-Policy**:\n",
    "   - Q-learning updates the action-value function using the assumption of optimal actions, regardless of the action actually taken by the behavior policy.\n",
    "\n",
    "2. **Exploration vs. Exploitation**:\n",
    "   - Often uses an **$ \\epsilon $-greedy policy** to ensure sufficient exploration while exploiting learned values.\n",
    "\n",
    "3. **Incremental Updates**:\n",
    "   - Updates $Q(s, a)$ incrementally after each step, making it efficient and scalable.\n",
    "\n",
    "---\n",
    "\n",
    "## Q-Learning Algorithm\n",
    "\n",
    "1. **Initialize**:\n",
    "   - Arbitrary action-value estimates $Q(s, a)$ (e.g., $Q(s, a) = 0$).\n",
    "   - Policy $\\pi(s)$ (e.g., $ \\epsilon $-greedy).\n",
    "\n",
    "2. **For Each Episode**:\n",
    "   - Initialize state $S_0$.\n",
    "\n",
    "3. **For Each Step in the Episode**:\n",
    "   - Choose action $A_t$ using an $ \\epsilon $-greedy policy.\n",
    "   - Take action $A_t$, observe $R_{t+1}$ and $S_{t+1}$.\n",
    "   - Update $Q(S_t, A_t)$:\n",
    "     $$\n",
    "     Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right]\n",
    "     $$\n",
    "   - Set $S_t = S_{t+1}$.\n",
    "\n",
    "4. **Repeat**:\n",
    "   - Continue until convergence of $Q(s, a)$ and the policy.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison: Q-Learning vs. SARSA\n",
    "\n",
    "| Feature                  | Q-Learning (Off-Policy)                                | SARSA (On-Policy)                                    |\n",
    "|--------------------------|--------------------------------------------------------|-----------------------------------------------------|\n",
    "| **Policy**               | Learns the optimal policy $\\pi^*$                      | Learns the policy being followed                   |\n",
    "| **Update Rule**          | Uses $\\max_a Q(S_{t+1}, a)$ for updates                | Uses $Q(S_{t+1}, A_{t+1})$ for updates             |\n",
    "| **Exploration**          | Exploitation dominates (due to $\\max$)                 | Balances exploration and exploitation              |\n",
    "| **Environment Suitability** | Efficient in deterministic tasks                     | Safer for stochastic environments                  |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Observations\n",
    "\n",
    "1. **Off-Policy Learning**:\n",
    "   - Q-learning learns the optimal policy independently of the exploration policy used.\n",
    "\n",
    "2. **Optimal Action Assumption**:\n",
    "   - Updates $Q(s, a)$ assuming the best action will always be taken in the future.\n",
    "\n",
    "3. **Incremental Updates**:\n",
    "   - Updates $Q(s, a)$ after each step, making it computationally efficient.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38fe0eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Policy:\n",
      "[3 1 1 1]\n",
      "[3 3 3 1]\n",
      "[3 3 3 1]\n",
      "[3 3 3 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Gridworld parameters\n",
    "grid_size = (4, 4)\n",
    "gamma = 0.9  # Discount factor\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1  # Exploration probability\n",
    "n_episodes = 500  # Number of episodes\n",
    "\n",
    "# Initialize action-value function\n",
    "Q = np.zeros((grid_size[0], grid_size[1], 4))  # 4 actions: up, down, left, right\n",
    "\n",
    "# Define ε-greedy policy\n",
    "def epsilon_greedy_policy(state, Q, epsilon):\n",
    "    i, j = state\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(4)  # Explore\n",
    "    else:\n",
    "        return np.argmax(Q[i, j])  # Exploit\n",
    "\n",
    "# Action mapping\n",
    "action_map = {\n",
    "    0: (-1, 0),  # up\n",
    "    1: (1, 0),   # down\n",
    "    2: (0, -1),  # left\n",
    "    3: (0, 1)    # right\n",
    "}\n",
    "\n",
    "# Step function\n",
    "def step(state, action):\n",
    "    i, j = state\n",
    "    di, dj = action_map[action]\n",
    "    ni, nj = i + di, j + dj\n",
    "    if 0 <= ni < grid_size[0] and 0 <= nj < grid_size[1]:  # Valid move\n",
    "        next_state = (ni, nj)\n",
    "    else:  # Hit a boundary\n",
    "        next_state = state\n",
    "    reward = -1 if next_state != (3, 3) else 10  # -1 per step, +10 for goal\n",
    "    done = next_state == (3, 3)\n",
    "    return next_state, reward, done\n",
    "\n",
    "# Q-Learning Algorithm\n",
    "def q_learning(Q, n_episodes, alpha, gamma, epsilon):\n",
    "    for episode in range(n_episodes):\n",
    "        state = (0, 0)  # Start state\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Choose action using ε-greedy policy\n",
    "            action = epsilon_greedy_policy(state, Q, epsilon)\n",
    "            # Take action, observe reward and next state\n",
    "            next_state, reward, done = step(state, action)\n",
    "            # Update Q(S_t, A_t)\n",
    "            i, j = state\n",
    "            ni, nj = next_state\n",
    "            Q[i, j, action] += alpha * (reward + gamma * np.max(Q[ni, nj]) - Q[i, j, action])\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "    return Q\n",
    "\n",
    "# Run Q-Learning\n",
    "Q = q_learning(Q, n_episodes, alpha, gamma, epsilon)\n",
    "\n",
    "# Derive policy\n",
    "policy = np.argmax(Q, axis=2)\n",
    "print(\"Learned Policy:\")\n",
    "for i in range(grid_size[0]):\n",
    "    print(policy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10d351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98242201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3439a471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff183b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0396a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186ad3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
